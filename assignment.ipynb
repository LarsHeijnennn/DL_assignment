{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa matplotlib mutagen pandas torch\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "train_folder = 'train'\n",
    "for f in os.listdir(train_folder):\n",
    "    if f.endswith('.wav'):\n",
    "        path = os.path.join(train_folder, f)\n",
    "        y, sr = librosa.load(path, sr=None)  # Load with original sample rate\n",
    "        if sr != 16000:\n",
    "            print(f\"{f} has sample rate {sr}\")\n",
    "\n",
    "# Nothings gets printed, so all files are 16000 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading an example file\n",
    "audio_path = 'Test set/3599.wav'\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title('Waveform of 3599.wav')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "train_folder = 'train'\n",
    "train_wav_files = [f for f in os.listdir(train_folder) if f.endswith('.wav')]\n",
    "print(f\"Number of .wav files in train folder: {len(train_wav_files)}\")\n",
    "\n",
    "total_duration = 0\n",
    "for f in train_wav_files:\n",
    "    y, sr = librosa.load(os.path.join(train_folder, f), sr=None)\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    total_duration += duration\n",
    "\n",
    "avg_duration = total_duration / len(train_wav_files)\n",
    "print(f\"Average duration: {avg_duration:.2f} seconds\")\n",
    "\n",
    "speaker_ids = {filename.split('_')[0] for filename in train_wav_files}\n",
    "male_speakers = [s for s in speaker_ids if s.endswith('m')]\n",
    "female_speakers = [s for s in speaker_ids if s.endswith('f')]\n",
    "\n",
    "print(f\"Total unique speakers: {len(speaker_ids)}\")\n",
    "print(f\"Male speakers: {len(male_speakers)}\")\n",
    "print(f\"Female speakers: {len(female_speakers)}\")\n",
    "print(\"\\nMale speaker IDs:\", sorted(male_speakers))\n",
    "print(\"Female speaker IDs:\", sorted(female_speakers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = []\n",
    "for f in train_wav_files:\n",
    "    y, sr = librosa.load(os.path.join(train_folder, f), sr=None)\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    durations.append(duration)\n",
    "\n",
    "min_duration = min(durations)\n",
    "max_duration = max(durations)\n",
    "std_duration = np.std(durations)\n",
    "\n",
    "print(f\"Minimum duration: {min_duration:.2f} seconds\")\n",
    "print(f\"Maximum duration: {max_duration:.2f} seconds\")\n",
    "print(f\"Standard deviation: {std_duration:.2f} seconds\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(durations, bins=30)\n",
    "plt.title('Distribution of Audio Durations')\n",
    "plt.xlabel('Duration (seconds)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting\n",
    "Skewed to the right (longer tail there). Most audio files between 3 and 7 sec. MOst common durations are around 4.5-6 sec. Few outliers longer than 10 sec, but rare. What to do with outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave  # Import the wave module for WAV file handling\n",
    "with wave.open('Test set/3599.wav', 'rb') as wav:  # Open WAV file in read-binary mode\n",
    "    print(f\"Channels: {wav.getnchannels()}\")  # Print number of audio channels\n",
    "    print(f\"Sample width: {wav.getsampwidth()}\")  # Print sample width in bytes - This shows how many bytes are used to store each audio sample (e.g., 2 bytes = 16-bit audio)\n",
    "    print(f\"Frame rate: {wav.getframerate()}\")  # Print sampling frequency\n",
    "    print(f\"Frames: {wav.getnframes()}\")  # Print total number of frames\n",
    "    print(f\"Parameters: {wav.getparams()}\")  # Print all WAV file parameters\n",
    "\n",
    "\n",
    "\n",
    "# The accent is encoded in the first character of the file name with a single number from 1 to 5. The gender is encoded as a single letter (’m’ or ’f’) corresponding to the second character of the file name.\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mutagen.wave import WAVE\n",
    "\n",
    "audio = WAVE('Test set/3599.wav')\n",
    "print(audio.tags)  # print None if there are no tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store our dataset information\n",
    "dataset_manifest = []\n",
    "\n",
    "for file_path in train_wav_files:\n",
    "    # Extract filename without extension\n",
    "    filename = os.path.basename(file_path).split('.')[0]\n",
    "    \n",
    "    # Extract accent (first character) and convert to 0-based index\n",
    "    accent = int(filename[0]) - 1  # Convert 1-5 to 0-4\n",
    "    \n",
    "    # Extract gender (second character)\n",
    "    gender = filename[1]\n",
    "    \n",
    "    # Create dictionary with file information\n",
    "    file_info = {\n",
    "        'file_path': file_path,\n",
    "        'accent': accent,\n",
    "        'gender': gender\n",
    "    }\n",
    "    \n",
    "    dataset_manifest.append(file_info)\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df_manifest = pd.DataFrame(dataset_manifest)\n",
    "\n",
    "# Display first few rows and basic statistics\n",
    "print(\"Dataset Manifest Preview:\")\n",
    "print(df_manifest.head())\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total files: {len(df_manifest)}\")\n",
    "print(\"\\nAccent distribution:\")\n",
    "print(df_manifest['accent'].value_counts().sort_index())\n",
    "print(\"\\nGender distribution:\")\n",
    "print(df_manifest['gender'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 file_path  \\\n",
      "0  /Users/larsheijnen/DL/Train/2m_9039.wav   \n",
      "1  /Users/larsheijnen/DL/Train/4f_1887.wav   \n",
      "2  /Users/larsheijnen/DL/Train/4f_9571.wav   \n",
      "3  /Users/larsheijnen/DL/Train/1m_3736.wav   \n",
      "4  /Users/larsheijnen/DL/Train/1m_3078.wav   \n",
      "\n",
      "                                            waveform  accent gender  \n",
      "0  [[tensor(-0.0001), tensor(-0.0001), tensor(-5....       2      m  \n",
      "1  [[tensor(0.), tensor(4.4749e-05), tensor(0.), ...       4      f  \n",
      "2  [[tensor(-0.0001), tensor(-0.0002), tensor(-0....       4      f  \n",
      "3  [[tensor(-0.0003), tensor(-0.0003), tensor(-0....       1      m  \n",
      "4  [[tensor(-0.0008), tensor(-0.0009), tensor(-0....       1      m  \n",
      "torch.Size([1, 41400])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_preprocess_audios_from_folder(folder_path, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Load and normalize all audio files in a folder using torchaudio, extracting accent and gender from filename.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to folder containing .wav files\n",
    "        target_sr (int): Sampling rate\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns ['file_path', 'waveform', 'accent', 'gender']\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith('.wav'):\n",
    "            file_path = os.path.join(folder_path, fname)\n",
    "            # Load audio\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "            # Normalize amplitude\n",
    "            waveform = waveform / waveform.abs().max()\n",
    "            # Extract accent and gender\n",
    "            accent = int(fname[0])  # 1-5\n",
    "            gender = fname[1]       # 'm' or 'f'\n",
    "            data.append({\n",
    "                'file_path': file_path,\n",
    "                'waveform': waveform,\n",
    "                'accent': accent,\n",
    "                'gender': gender\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = load_and_preprocess_audios_from_folder(\"/Users/larsheijnen/DL/Train\")\n",
    "print(df.head())\n",
    "\n",
    "#Size first waveform\n",
    "print(df['waveform'].iloc[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Convert accent from 1-5 to 0-4\n",
    "if 'accent' in df.columns:\n",
    "    df['accent_label'] = df['accent'].apply(lambda x: int(x) - 1)\n",
    "else:\n",
    "    print(\"Warning: 'accent' column not found in DataFrame. Accent prediction will not work.\")\n",
    "    # Add a dummy label if you want to proceed with model structure testing\n",
    "    df['accent_label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Device Configuration ---\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Voor macos met M-chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. PyTorch Dataset ---\n",
    "class AccentDatasetRNN(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Waveform is already a tensor [1, length] from your preprocessing\n",
    "        waveform = self.df.iloc[idx]['waveform'].squeeze(0) # RNN expects [seq_len, features] or [batch, seq_len, features]\n",
    "                                                           # Here, seq_len is audio length, features is 1 (mono)\n",
    "        accent_label = self.df.iloc[idx]['accent']\n",
    "        # Convert accent 1-5 to 0-4 for CrossEntropyLoss\n",
    "        label = torch.tensor(accent_label - 1, dtype=torch.long)\n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Custom Collate Function ---\n",
    "def collate_fn_rnn(batch):\n",
    "    # batch is a list of tuples (waveform_tensor, label_tensor)\n",
    "    waveforms = [item[0] for item in batch]\n",
    "    labels = torch.stack([item[1] for item in batch])\n",
    "    \n",
    "    # Get lengths of each sequence\n",
    "    lengths = torch.tensor([len(w) for w in waveforms])\n",
    "\n",
    "    # Pad sequences in this batch (batch_first=False for pack_padded_sequence easier handling with RNN)\n",
    "    # pad_sequence expects a list of tensors, each tensor is a sequence\n",
    "    padded_waveforms = pad_sequence(waveforms, batch_first=False, padding_value=0.0)\n",
    "    # padded_waveforms will be of shape (max_seq_len_in_batch, batch_size)\n",
    "    # We need it as (max_seq_len_in_batch, batch_size, num_features=1) for RNN\n",
    "    padded_waveforms = padded_waveforms.unsqueeze(-1) # Add feature dimension\n",
    "\n",
    "    return padded_waveforms, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. RNN Model Definition ---\n",
    "class AudioRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, num_classes=5, rnn_type='LSTM', dropout=0.3):\n",
    "        super(AudioRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type.upper()\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                               batch_first=False, dropout=dropout if num_layers > 1 else 0)\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                              batch_first=False, dropout=dropout if num_layers > 1 else 0)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x shape: (max_seq_len, batch_size, input_size)\n",
    "        # lengths shape: (batch_size)\n",
    "\n",
    "        # Pack sequence\n",
    "        # Enforce_sorted=False because DataLoader with shuffle=True might not guarantee order\n",
    "        # Sorting is done within collate_fn if needed, but pack_padded_sequence can handle unsorted if lengths are provided\n",
    "        packed_input = pack_padded_sequence(x, lengths.cpu(), batch_first=False, enforce_sorted=False)\n",
    "        \n",
    "        # RNN forward pass\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            packed_output, (hn, cn) = self.rnn(packed_input)\n",
    "        else: # GRU\n",
    "            packed_output, hn = self.rnn(packed_input)\n",
    "        \n",
    "        # We can use the hidden state of the last layer (hn)\n",
    "        # hn is (num_layers, batch_size, hidden_size)\n",
    "        # We want the output of the last RNN layer for each sequence\n",
    "        # Take the hidden state of the last layer:\n",
    "        last_hidden = hn[-1] # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        out = self.dropout(last_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on mps with batch size 16...\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Training Setup ---\n",
    "# Hyperparameters\n",
    "input_size = 1  # Mono audio\n",
    "hidden_size = 128 # Can be tuned\n",
    "num_rnn_layers = 2 # Can be tuned\n",
    "num_classes = 5 # 5 accent classes\n",
    "learning_rate = 0.001\n",
    "batch_size = 16 # Adjust based on your 8GB RAM. Start small.\n",
    "num_epochs = 10 # Start with a few epochs\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "# Ensure 'df' is your DataFrame with 'waveform' and 'accent' columns\n",
    "if 'df' not in locals():\n",
    "    print(\"DataFrame 'df' not found. Please load your data.\")\n",
    "    # df = load_and_preprocess_audios_from_folder(\"/Users/larsheijnen/DL/Train\") # Placeholder\n",
    "else:\n",
    "    train_dataset = AccentDatasetRNN(df)\n",
    "    train_loader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, \n",
    "                              collate_fn=collate_fn_rnn,\n",
    "                              pin_memory=False, # pin_memory is more relevant for CUDA\n",
    "                              num_workers=0 # <--- CHANGE THIS\n",
    "                             )\n",
    "\n",
    "    # Instantiate model, loss, and optimizer\n",
    "    model = AudioRNN(input_size=input_size, \n",
    "                     hidden_size=hidden_size, \n",
    "                     num_layers=num_rnn_layers, \n",
    "                     num_classes=num_classes,\n",
    "                     rnn_type='GRU' # GRU is often a bit faster and lighter than LSTM\n",
    "                    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"Starting training on {device} with batch size {batch_size}...\")\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, (waveforms, lengths, labels) in enumerate(train_loader):\n",
    "            waveforms = waveforms.to(device) \n",
    "            # lengths are already tensors, should be on CPU for pack_padded_sequence\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(waveforms, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 10 == 0: # Print every 10 batches\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] completed. Average Training Loss: {avg_loss:.4f}')\n",
    "\n",
    "    print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
