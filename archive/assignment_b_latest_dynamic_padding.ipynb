{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AccentSpectrogramDataset(Dataset):\n",
    "    def __init__(self, folder_path,\n",
    "                 target_sr: int = 16000,\n",
    "                 use_mel: bool = False,\n",
    "                 n_fft: int = 400,\n",
    "                 hop_length: int = None,\n",
    "                 n_mels: int = 64,\n",
    "                 log_scale: bool = True):\n",
    "        # store file paths only; transform per item\n",
    "        self.file_paths = [\n",
    "            os.path.join(folder_path, f)\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.endswith('.wav')\n",
    "        ]\n",
    "        self.target_sr = target_sr\n",
    "        self.use_mel = use_mel\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length or n_fft // 2\n",
    "        self.n_mels = n_mels\n",
    "        self.log_scale = log_scale\n",
    "\n",
    "        # pre-configure transform funct\n",
    "        if self.use_mel:\n",
    "            self._transform = lambda w: T.MelSpectrogram(\n",
    "                sample_rate=self.target_sr,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length,\n",
    "                n_mels=self.n_mels\n",
    "            )(w)\n",
    "        else:\n",
    "            self._transform = lambda w: T.Spectrogram(\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length\n",
    "            )(w)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        if sr != self.target_sr:\n",
    "            waveform = T.Resample(sr, self.target_sr)(waveform)\n",
    "\n",
    "        spec = self._transform(waveform)\n",
    "        if self.log_scale:\n",
    "            spec = torch.log(spec + 1e-6)\n",
    "\n",
    "        fname = os.path.basename(path)\n",
    "        accent = int(fname[0]) - 1          # classes 0â€“4\n",
    "\n",
    "        return spec, accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# def pad_collate(batch):\n",
    "#     specs, accents = zip(*batch)\n",
    "\n",
    "#     max_len = max([s.shape[-1] for s in specs])\n",
    "#     padded_specs = []\n",
    "\n",
    "#     for s in specs:\n",
    "#         pad_amount = max_len - s.shape[-1]\n",
    "#         padded = F.pad(s, (0, pad_amount))\n",
    "#         padded_specs.append(padded)\n",
    "\n",
    "    # return (\n",
    "#         torch.stack(padded_specs),             # [B, 1, Freq, Time]\n",
    "#         torch.tensor(accents),                # [B]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #baseline\n",
    "\n",
    "# def pad_collate(batch, target_width=208):\n",
    "#     specs, accents = zip(*batch)\n",
    "#     padded_specs = []\n",
    "#     for s in specs:\n",
    "#         pad_amount = target_width - s.shape[-1]\n",
    "#         if pad_amount > 0:\n",
    "#             padded = F.pad(s, (0, pad_amount))\n",
    "#         else:\n",
    "#             padded = s[..., :target_width]  # crop if too long\n",
    "#         padded_specs.append(padded)\n",
    "#     return (\n",
    "#         torch.stack(padded_specs),\n",
    "#         torch.tensor(accents),\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (spec, label) tuples, \n",
    "    where spec.shape == [1, FreqBins, TimeFrames]\n",
    "    \"\"\"\n",
    "    specs, accents = zip(*batch)\n",
    "\n",
    "    # 1) find the longest time-axis in this batch\n",
    "    max_len = max(s.shape[-1] for s in specs)\n",
    "\n",
    "    # 2) pad each waveform up to max_len\n",
    "    padded_specs = []\n",
    "    for s in specs:\n",
    "        pad_amt = max_len - s.shape[-1]\n",
    "        # pad on the right of time-axis\n",
    "        padded = F.pad(s, (0, pad_amt))\n",
    "        padded_specs.append(padded)\n",
    "\n",
    "    # 3) stack into [B, 1, F, T_batch]\n",
    "    batch_specs = torch.stack(padded_specs)\n",
    "    batch_labels = torch.tensor(accents, dtype=torch.long)\n",
    "    return batch_specs, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3166\n",
      "Spectrogram shape: torch.Size([1, 201, 526])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "#dataset = AccentSpectrogramDataset(\"/Users/larsheijnen/DL/Train\")\n",
    "dataset = AccentSpectrogramDataset(\"/Users/larsheijnen/DL/Train\")\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "\n",
    "# Look at shape of first spectrogram\n",
    "x, y= dataset[6]\n",
    "print(f\"Spectrogram shape: {x.shape}\")\n",
    "print(f\"Label: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrograms: torch.Size([4, 1, 201, 772])\n",
      "Accents: tensor([1, 0, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Use batch_size=4 for low RAM, pin_memory is False for macOS/MPS\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=pad_collate, pin_memory=False)\n",
    "\n",
    "# Try again\n",
    "for batch in dataloader:\n",
    "    spectrograms, accents = batch\n",
    "    print(f\"Spectrograms: {spectrograms.shape}\")  # (B, 1, F, T)\n",
    "    print(f\"Accents: {accents}\")                  # (B,)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# All models now share dynamic time-agnostic pooling:\n",
    "# - feature extractor: conv layers (with optional BN)\n",
    "# - self.global_pool: nn.AdaptiveAvgPool2d((1,1))\n",
    "# - classifier: nn.Linear(C_out, num_classes)\n",
    "\n",
    "# Model 1: baseline\n",
    "class CNNBaseline(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)           # [B,32,F',T']\n",
    "        x = self.global_pool(x)        # [B,32,1,1]\n",
    "        x = x.view(x.size(0), -1)      # [B,32]\n",
    "        return self.fc(x)\n",
    "\n",
    "# Model 2: baseline + batch normalization\n",
    "class CNNBaseline_BatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Model 3: baseline + dropout 0.3\n",
    "class CNNBaseline_Dropout3(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Model 4: baseline + dropout 0.5\n",
    "class CNNBaseline_Dropout5(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Model 5: baseline + batch norm + dropout 0.3\n",
    "class CNNBaseline_Dropout3_BatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Model 6: baseline + batch norm + dropout 0.5\n",
    "class CNNBaseline_Dropout5_BatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AccentCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(8)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(16)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(32)\n",
    "        self.pool3 = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.fc = nn.Linear(32, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)               # â†’ (B, 32)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"Model1\": CNNBaseline,\n",
    "    \"Model2\": CNNBaseline_BatchNorm, \n",
    "    \"Model3\": CNNBaseline_Dropout3,\n",
    "    \"Model4\": CNNBaseline_Dropout5,\n",
    "    \"Model5\": CNNBaseline_Dropout3_BatchNorm,\n",
    "    \"Model6\": CNNBaseline_Dropout5_BatchNorm,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training model: CNNBaseline ===\n",
      "Epoch  1 | Train Loss: 252.695 | Train Acc: 29.58% | Train Prec: 11.81% | Train Recall: 24.84% | Train F1: 15.98% || Test Acc: 28.71% | Test Prec: 11.47% | Test Recall: 25.22% | Test F1: 15.76%\n",
      "Epoch  2 | Train Loss: 249.714 | Train Acc: 30.41% | Train Prec: 12.81% | Train Recall: 25.56% | Train F1: 15.85% || Test Acc: 26.81% | Test Prec: 10.72% | Test Recall: 24.16% | Test F1: 13.75%\n",
      "Epoch  3 | Train Loss: 246.125 | Train Acc: 33.65% | Train Prec: 23.58% | Train Recall: 28.78% | Train F1: 20.16% || Test Acc: 29.81% | Test Prec: 16.96% | Test Recall: 26.72% | Test F1: 17.43%\n",
      "Epoch  4 | Train Loss: 245.085 | Train Acc: 34.36% | Train Prec: 31.21% | Train Recall: 30.18% | Train F1: 22.38% || Test Acc: 30.28% | Test Prec: 18.09% | Test Recall: 27.83% | Test F1: 19.34%\n",
      "Epoch  5 | Train Loss: 241.453 | Train Acc: 36.10% | Train Prec: 35.76% | Train Recall: 32.33% | Train F1: 27.11% || Test Acc: 34.07% | Test Prec: 36.57% | Test Recall: 31.53% | Test F1: 25.77%\n",
      "Epoch  6 | Train Loss: 238.530 | Train Acc: 36.81% | Train Prec: 27.87% | Train Recall: 33.44% | Train F1: 27.66% || Test Acc: 34.23% | Test Prec: 28.66% | Test Recall: 31.73% | Test F1: 26.46%\n",
      "Epoch  7 | Train Loss: 233.489 | Train Acc: 40.88% | Train Prec: 40.53% | Train Recall: 36.92% | Train F1: 30.75% || Test Acc: 36.44% | Test Prec: 33.74% | Test Recall: 33.83% | Test F1: 27.07%\n",
      "Epoch  8 | Train Loss: 227.998 | Train Acc: 41.55% | Train Prec: 31.14% | Train Recall: 36.63% | Train F1: 31.28% || Test Acc: 37.70% | Test Prec: 28.08% | Test Recall: 34.43% | Test F1: 28.39%\n",
      "Epoch  9 | Train Loss: 222.981 | Train Acc: 41.94% | Train Prec: 29.85% | Train Recall: 38.62% | Train F1: 32.20% || Test Acc: 38.96% | Test Prec: 31.20% | Test Recall: 36.58% | Test F1: 30.80%\n",
      "Epoch 10 | Train Loss: 218.610 | Train Acc: 37.72% | Train Prec: 25.78% | Train Recall: 34.15% | Train F1: 26.65% || Test Acc: 34.23% | Test Prec: 24.50% | Test Recall: 32.00% | Test F1: 24.48%\n",
      "Epoch 11 | Train Loss: 215.838 | Train Acc: 43.76% | Train Prec: 40.66% | Train Recall: 38.70% | Train F1: 33.50% || Test Acc: 39.91% | Test Prec: 34.08% | Test Recall: 36.45% | Test F1: 30.13%\n",
      "Epoch 12 | Train Loss: 212.010 | Train Acc: 44.59% | Train Prec: 42.24% | Train Recall: 41.85% | Train F1: 39.66% || Test Acc: 41.80% | Test Prec: 40.16% | Test Recall: 39.89% | Test F1: 37.93%\n",
      "Epoch 13 | Train Loss: 208.015 | Train Acc: 41.82% | Train Prec: 39.05% | Train Recall: 38.63% | Train F1: 33.73% || Test Acc: 39.27% | Test Prec: 42.08% | Test Recall: 37.41% | Test F1: 33.20%\n",
      "Epoch 14 | Train Loss: 205.259 | Train Acc: 45.93% | Train Prec: 44.67% | Train Recall: 41.27% | Train F1: 35.82% || Test Acc: 40.06% | Test Prec: 48.78% | Test Recall: 37.15% | Test F1: 31.39%\n",
      "Epoch 15 | Train Loss: 201.053 | Train Acc: 45.77% | Train Prec: 43.32% | Train Recall: 41.36% | Train F1: 35.54% || Test Acc: 41.32% | Test Prec: 42.88% | Test Recall: 38.16% | Test F1: 33.17%\n",
      "Epoch 16 | Train Loss: 199.564 | Train Acc: 47.67% | Train Prec: 44.13% | Train Recall: 42.31% | Train F1: 38.30% || Test Acc: 44.01% | Test Prec: 48.35% | Test Recall: 40.61% | Test F1: 36.56%\n",
      "Epoch 17 | Train Loss: 197.607 | Train Acc: 49.21% | Train Prec: 47.01% | Train Recall: 45.89% | Train F1: 44.28% || Test Acc: 45.74% | Test Prec: 45.36% | Test Recall: 44.10% | Test F1: 42.38%\n",
      "Epoch 18 | Train Loss: 192.490 | Train Acc: 52.09% | Train Prec: 51.43% | Train Recall: 47.34% | Train F1: 44.10% || Test Acc: 47.00% | Test Prec: 48.73% | Test Recall: 44.79% | Test F1: 41.02%\n",
      "Epoch 19 | Train Loss: 192.637 | Train Acc: 51.11% | Train Prec: 50.48% | Train Recall: 45.34% | Train F1: 41.30% || Test Acc: 47.00% | Test Prec: 46.97% | Test Recall: 43.32% | Test F1: 38.74%\n",
      "Epoch 20 | Train Loss: 188.532 | Train Acc: 54.27% | Train Prec: 52.20% | Train Recall: 49.89% | Train F1: 46.83% || Test Acc: 50.00% | Test Prec: 50.03% | Test Recall: 47.94% | Test F1: 44.61%\n",
      "\n",
      "=== Training model: CNNBaseline_BatchNorm ===\n",
      "Epoch  1 | Train Loss: 249.291 | Train Acc: 31.83% | Train Prec: 19.88% | Train Recall: 27.32% | Train F1: 21.20% || Test Acc: 29.34% | Test Prec: 17.16% | Test Recall: 26.64% | Test F1: 19.24%\n",
      "Epoch  2 | Train Loss: 244.596 | Train Acc: 27.92% | Train Prec: 26.57% | Train Recall: 26.30% | Train F1: 19.64% || Test Acc: 24.13% | Test Prec: 25.10% | Test Recall: 23.83% | Test F1: 17.51%\n",
      "Epoch  3 | Train Loss: 243.458 | Train Acc: 31.00% | Train Prec: 27.05% | Train Recall: 28.48% | Train F1: 23.28% || Test Acc: 26.97% | Test Prec: 25.79% | Test Recall: 26.27% | Test F1: 20.67%\n",
      "Epoch  4 | Train Loss: 240.108 | Train Acc: 25.43% | Train Prec: 31.86% | Train Recall: 24.28% | Train F1: 15.73% || Test Acc: 23.03% | Test Prec: 30.40% | Test Recall: 23.62% | Test F1: 14.85%\n",
      "Epoch  5 | Train Loss: 237.573 | Train Acc: 35.86% | Train Prec: 28.67% | Train Recall: 31.72% | Train F1: 26.08% || Test Acc: 32.65% | Test Prec: 46.27% | Test Recall: 30.15% | Test F1: 23.80%\n",
      "Epoch  6 | Train Loss: 235.462 | Train Acc: 25.91% | Train Prec: 30.83% | Train Recall: 22.36% | Train F1: 13.14% || Test Acc: 23.34% | Test Prec: 26.81% | Test Recall: 21.55% | Test F1: 11.00%\n",
      "Epoch  7 | Train Loss: 232.246 | Train Acc: 34.48% | Train Prec: 37.90% | Train Recall: 30.47% | Train F1: 23.55% || Test Acc: 31.07% | Test Prec: 37.17% | Test Recall: 29.38% | Test F1: 21.95%\n",
      "Epoch  8 | Train Loss: 226.650 | Train Acc: 38.59% | Train Prec: 28.53% | Train Recall: 33.43% | Train F1: 27.07% || Test Acc: 35.17% | Test Prec: 28.37% | Test Recall: 31.82% | Test F1: 24.74%\n",
      "Epoch  9 | Train Loss: 223.549 | Train Acc: 38.82% | Train Prec: 29.30% | Train Recall: 33.72% | Train F1: 27.95% || Test Acc: 35.49% | Test Prec: 27.72% | Test Recall: 31.83% | Test F1: 25.69%\n",
      "Epoch 10 | Train Loss: 217.656 | Train Acc: 44.15% | Train Prec: 30.90% | Train Recall: 38.28% | Train F1: 30.99% || Test Acc: 39.27% | Test Prec: 26.93% | Test Recall: 35.50% | Test F1: 26.98%\n",
      "Epoch 11 | Train Loss: 212.543 | Train Acc: 39.85% | Train Prec: 35.89% | Train Recall: 34.11% | Train F1: 24.67% || Test Acc: 35.17% | Test Prec: 16.35% | Test Recall: 31.38% | Test F1: 20.16%\n",
      "Epoch 12 | Train Loss: 208.352 | Train Acc: 52.41% | Train Prec: 40.65% | Train Recall: 46.18% | Train F1: 38.79% || Test Acc: 46.69% | Test Prec: 39.54% | Test Recall: 43.28% | Test F1: 35.90%\n",
      "Epoch 13 | Train Loss: 203.715 | Train Acc: 43.48% | Train Prec: 45.99% | Train Recall: 42.13% | Train F1: 37.33% || Test Acc: 41.80% | Test Prec: 44.52% | Test Recall: 41.71% | Test F1: 37.57%\n",
      "Epoch 14 | Train Loss: 202.224 | Train Acc: 48.42% | Train Prec: 42.77% | Train Recall: 42.71% | Train F1: 39.16% || Test Acc: 43.22% | Test Prec: 42.03% | Test Recall: 39.65% | Test F1: 36.49%\n",
      "Epoch 15 | Train Loss: 197.061 | Train Acc: 53.91% | Train Prec: 48.19% | Train Recall: 48.87% | Train F1: 44.87% || Test Acc: 49.21% | Test Prec: 45.00% | Test Recall: 46.18% | Test F1: 41.29%\n",
      "Epoch 16 | Train Loss: 193.109 | Train Acc: 49.88% | Train Prec: 52.89% | Train Recall: 46.46% | Train F1: 43.83% || Test Acc: 47.48% | Test Prec: 53.63% | Test Recall: 46.24% | Test F1: 43.12%\n",
      "Epoch 17 | Train Loss: 188.658 | Train Acc: 58.29% | Train Prec: 54.49% | Train Recall: 53.28% | Train F1: 49.55% || Test Acc: 53.15% | Test Prec: 50.37% | Test Recall: 50.55% | Test F1: 46.86%\n",
      "Epoch 18 | Train Loss: 187.212 | Train Acc: 59.12% | Train Prec: 54.43% | Train Recall: 54.71% | Train F1: 53.30% || Test Acc: 52.05% | Test Prec: 48.54% | Test Recall: 49.51% | Test F1: 47.87%\n",
      "Epoch 19 | Train Loss: 183.335 | Train Acc: 49.57% | Train Prec: 39.83% | Train Recall: 43.62% | Train F1: 36.92% || Test Acc: 45.11% | Test Prec: 40.19% | Test Recall: 40.98% | Test F1: 33.02%\n",
      "Epoch 20 | Train Loss: 178.657 | Train Acc: 51.86% | Train Prec: 48.95% | Train Recall: 45.89% | Train F1: 40.43% || Test Acc: 46.37% | Test Prec: 37.22% | Test Recall: 42.49% | Test F1: 36.55%\n",
      "\n",
      "=== Training model: CNNBaseline_Dropout3 ===\n",
      "Epoch  1 | Train Loss: 253.743 | Train Acc: 29.03% | Train Prec: 11.51% | Train Recall: 24.37% | Train F1: 15.43% || Test Acc: 27.76% | Test Prec: 11.13% | Test Recall: 24.27% | Test F1: 15.15%\n",
      "Epoch  2 | Train Loss: 251.810 | Train Acc: 30.69% | Train Prec: 12.37% | Train Recall: 25.77% | Train F1: 16.64% || Test Acc: 27.60% | Test Prec: 11.08% | Test Recall: 24.47% | Test F1: 15.14%\n",
      "Epoch  3 | Train Loss: 251.361 | Train Acc: 30.37% | Train Prec: 12.22% | Train Recall: 25.51% | Train F1: 16.48% || Test Acc: 27.92% | Test Prec: 11.19% | Test Recall: 24.72% | Test F1: 15.33%\n",
      "Epoch  4 | Train Loss: 250.142 | Train Acc: 26.82% | Train Prec: 23.76% | Train Recall: 25.05% | Train F1: 22.09% || Test Acc: 24.29% | Test Prec: 23.26% | Test Recall: 23.54% | Test F1: 20.28%\n",
      "Epoch  5 | Train Loss: 249.147 | Train Acc: 30.25% | Train Prec: 12.09% | Train Recall: 25.40% | Train F1: 16.28% || Test Acc: 29.50% | Test Prec: 11.83% | Test Recall: 25.81% | Test F1: 16.13%\n",
      "Epoch  6 | Train Loss: 249.807 | Train Acc: 30.73% | Train Prec: 27.25% | Train Recall: 26.84% | Train F1: 21.07% || Test Acc: 29.65% | Test Prec: 26.37% | Test Recall: 26.56% | Test F1: 20.11%\n",
      "Epoch  7 | Train Loss: 248.173 | Train Acc: 29.38% | Train Prec: 22.31% | Train Recall: 25.11% | Train F1: 17.59% || Test Acc: 29.02% | Test Prec: 19.06% | Test Recall: 25.49% | Test F1: 17.00%\n",
      "Epoch  8 | Train Loss: 249.261 | Train Acc: 30.06% | Train Prec: 28.20% | Train Recall: 25.79% | Train F1: 19.59% || Test Acc: 27.76% | Test Prec: 19.98% | Test Recall: 24.47% | Test F1: 17.55%\n",
      "Epoch  9 | Train Loss: 248.743 | Train Acc: 34.40% | Train Prec: 31.96% | Train Recall: 30.25% | Train F1: 25.65% || Test Acc: 29.81% | Test Prec: 24.57% | Test Recall: 27.34% | Test F1: 22.01%\n",
      "Epoch 10 | Train Loss: 248.201 | Train Acc: 33.45% | Train Prec: 40.79% | Train Recall: 29.50% | Train F1: 23.45% || Test Acc: 29.97% | Test Prec: 53.62% | Test Recall: 27.48% | Test F1: 21.04%\n",
      "Epoch 11 | Train Loss: 247.978 | Train Acc: 32.58% | Train Prec: 24.06% | Train Recall: 30.11% | Train F1: 23.51% || Test Acc: 29.50% | Test Prec: 17.34% | Test Recall: 27.38% | Test F1: 21.04%\n",
      "Epoch 12 | Train Loss: 248.042 | Train Acc: 32.35% | Train Prec: 27.52% | Train Recall: 27.55% | Train F1: 20.44% || Test Acc: 29.18% | Test Prec: 17.21% | Test Recall: 26.32% | Test F1: 18.74%\n",
      "Epoch 13 | Train Loss: 246.259 | Train Acc: 32.35% | Train Prec: 31.09% | Train Recall: 28.27% | Train F1: 23.41% || Test Acc: 29.34% | Test Prec: 23.79% | Test Recall: 26.67% | Test F1: 22.00%\n",
      "Epoch 14 | Train Loss: 246.583 | Train Acc: 34.79% | Train Prec: 29.77% | Train Recall: 31.38% | Train F1: 27.90% || Test Acc: 31.23% | Test Prec: 25.84% | Test Recall: 28.89% | Test F1: 25.10%\n",
      "Epoch 15 | Train Loss: 245.446 | Train Acc: 36.26% | Train Prec: 31.49% | Train Recall: 32.55% | Train F1: 28.55% || Test Acc: 30.60% | Test Prec: 26.06% | Test Recall: 28.84% | Test F1: 24.07%\n",
      "Epoch 16 | Train Loss: 244.347 | Train Acc: 31.40% | Train Prec: 53.53% | Train Recall: 27.20% | Train F1: 19.92% || Test Acc: 28.39% | Test Prec: 29.14% | Test Recall: 25.88% | Test F1: 17.18%\n",
      "Epoch 17 | Train Loss: 243.604 | Train Acc: 36.97% | Train Prec: 41.52% | Train Recall: 32.99% | Train F1: 29.44% || Test Acc: 32.33% | Test Prec: 38.62% | Test Recall: 29.99% | Test F1: 25.96%\n",
      "Epoch 18 | Train Loss: 243.177 | Train Acc: 37.68% | Train Prec: 32.41% | Train Recall: 33.72% | Train F1: 29.68% || Test Acc: 32.33% | Test Prec: 26.29% | Test Recall: 30.07% | Test F1: 25.34%\n",
      "Epoch 19 | Train Loss: 242.123 | Train Acc: 35.74% | Train Prec: 32.13% | Train Recall: 31.16% | Train F1: 25.71% || Test Acc: 31.70% | Test Prec: 27.11% | Test Recall: 28.96% | Test F1: 22.95%\n",
      "Epoch 20 | Train Loss: 240.578 | Train Acc: 36.85% | Train Prec: 30.41% | Train Recall: 32.74% | Train F1: 27.73% || Test Acc: 33.12% | Test Prec: 30.27% | Test Recall: 31.22% | Test F1: 25.86%\n",
      "\n",
      "=== Training model: CNNBaseline_Dropout5 ===\n",
      "Epoch  1 | Train Loss: 253.953 | Train Acc: 22.79% | Train Prec:  6.94% | Train Recall: 19.11% | Train F1:  8.66% || Test Acc: 23.19% | Test Prec:  6.93% | Test Recall: 19.66% | Test F1:  8.79%\n",
      "Epoch  2 | Train Loss: 252.642 | Train Acc: 30.17% | Train Prec: 12.03% | Train Recall: 25.33% | Train F1: 16.27% || Test Acc: 28.39% | Test Prec: 11.34% | Test Recall: 24.89% | Test F1: 15.55%\n",
      "Epoch  3 | Train Loss: 251.904 | Train Acc: 28.12% | Train Prec: 11.11% | Train Recall: 23.60% | Train F1: 14.54% || Test Acc: 26.81% | Test Prec: 10.86% | Test Recall: 23.25% | Test F1: 14.24%\n",
      "Epoch  4 | Train Loss: 252.250 | Train Acc: 30.53% | Train Prec: 12.41% | Train Recall: 25.65% | Train F1: 16.43% || Test Acc: 27.29% | Test Prec: 10.95% | Test Recall: 24.32% | Test F1: 14.80%\n",
      "Epoch  5 | Train Loss: 250.552 | Train Acc: 29.11% | Train Prec: 12.53% | Train Recall: 24.47% | Train F1: 14.96% || Test Acc: 26.03% | Test Prec: 10.66% | Test Recall: 23.52% | Test F1: 13.06%\n",
      "Epoch  6 | Train Loss: 250.377 | Train Acc: 30.96% | Train Prec: 12.57% | Train Recall: 26.01% | Train F1: 16.67% || Test Acc: 27.60% | Test Prec: 11.11% | Test Recall: 24.57% | Test F1: 15.02%\n",
      "Epoch  7 | Train Loss: 249.974 | Train Acc: 29.86% | Train Prec: 31.95% | Train Recall: 25.08% | Train F1: 16.12% || Test Acc: 29.34% | Test Prec: 11.80% | Test Recall: 25.65% | Test F1: 16.02%\n",
      "Epoch  8 | Train Loss: 249.100 | Train Acc: 32.31% | Train Prec: 18.51% | Train Recall: 27.27% | Train F1: 18.67% || Test Acc: 30.28% | Test Prec: 18.29% | Test Recall: 27.05% | Test F1: 17.61%\n",
      "Epoch  9 | Train Loss: 247.659 | Train Acc: 33.02% | Train Prec: 18.87% | Train Recall: 28.00% | Train F1: 20.31% || Test Acc: 29.81% | Test Prec: 15.71% | Test Recall: 26.55% | Test F1: 18.06%\n",
      "Epoch 10 | Train Loss: 248.654 | Train Acc: 32.23% | Train Prec: 18.84% | Train Recall: 27.44% | Train F1: 20.61% || Test Acc: 28.23% | Test Prec: 15.41% | Test Recall: 24.90% | Test F1: 17.75%\n",
      "Epoch 11 | Train Loss: 246.083 | Train Acc: 33.18% | Train Prec: 19.33% | Train Recall: 28.19% | Train F1: 21.00% || Test Acc: 28.71% | Test Prec: 15.65% | Test Recall: 25.25% | Test F1: 17.59%\n",
      "Epoch 12 | Train Loss: 245.492 | Train Acc: 34.12% | Train Prec: 19.73% | Train Recall: 29.03% | Train F1: 21.00% || Test Acc: 31.07% | Test Prec: 18.03% | Test Recall: 28.20% | Test F1: 19.63%\n",
      "Epoch 13 | Train Loss: 244.233 | Train Acc: 37.72% | Train Prec: 22.05% | Train Recall: 32.50% | Train F1: 25.07% || Test Acc: 32.65% | Test Prec: 18.37% | Test Recall: 29.87% | Test F1: 21.76%\n",
      "Epoch 14 | Train Loss: 243.051 | Train Acc: 38.19% | Train Prec: 22.19% | Train Recall: 32.62% | Train F1: 25.34% || Test Acc: 33.91% | Test Prec: 19.25% | Test Recall: 30.37% | Test F1: 22.73%\n",
      "Epoch 15 | Train Loss: 241.102 | Train Acc: 40.72% | Train Prec: 23.89% | Train Recall: 35.28% | Train F1: 28.29% || Test Acc: 35.65% | Test Prec: 20.67% | Test Recall: 32.54% | Test F1: 25.11%\n",
      "Epoch 16 | Train Loss: 239.147 | Train Acc: 40.76% | Train Prec: 23.14% | Train Recall: 35.07% | Train F1: 27.08% || Test Acc: 35.65% | Test Prec: 19.81% | Test Recall: 32.47% | Test F1: 23.90%\n",
      "Epoch 17 | Train Loss: 237.399 | Train Acc: 40.88% | Train Prec: 28.56% | Train Recall: 35.30% | Train F1: 27.51% || Test Acc: 35.49% | Test Prec: 19.86% | Test Recall: 32.39% | Test F1: 24.02%\n",
      "Epoch 18 | Train Loss: 236.071 | Train Acc: 44.19% | Train Prec: 25.68% | Train Recall: 38.00% | Train F1: 30.16% || Test Acc: 40.22% | Test Prec: 23.38% | Test Recall: 36.56% | Test F1: 28.15%\n",
      "Epoch 19 | Train Loss: 230.141 | Train Acc: 43.72% | Train Prec: 25.18% | Train Recall: 37.92% | Train F1: 30.09% || Test Acc: 38.49% | Test Prec: 42.02% | Test Recall: 35.43% | Test F1: 27.34%\n",
      "Epoch 20 | Train Loss: 227.447 | Train Acc: 45.93% | Train Prec: 36.02% | Train Recall: 39.68% | Train F1: 32.45% || Test Acc: 42.74% | Test Prec: 46.17% | Test Recall: 39.03% | Test F1: 31.45%\n",
      "\n",
      "=== Training model: CNNBaseline_Dropout3_BatchNorm ===\n",
      "Epoch  1 | Train Loss: 248.571 | Train Acc: 32.50% | Train Prec: 23.01% | Train Recall: 27.79% | Train F1: 19.84% || Test Acc: 29.18% | Test Prec: 20.48% | Test Recall: 26.15% | Test F1: 17.83%\n",
      "Epoch  2 | Train Loss: 244.017 | Train Acc: 33.14% | Train Prec: 28.77% | Train Recall: 29.10% | Train F1: 23.76% || Test Acc: 31.23% | Test Prec: 27.39% | Test Recall: 29.23% | Test F1: 22.72%\n",
      "Epoch  3 | Train Loss: 244.235 | Train Acc: 36.69% | Train Prec: 29.41% | Train Recall: 33.90% | Train F1: 31.33% || Test Acc: 33.60% | Test Prec: 27.08% | Test Recall: 31.69% | Test F1: 28.88%\n",
      "Epoch  4 | Train Loss: 241.362 | Train Acc: 39.53% | Train Prec: 41.81% | Train Recall: 33.90% | Train F1: 26.41% || Test Acc: 34.54% | Test Prec: 18.62% | Test Recall: 30.97% | Test F1: 22.72%\n",
      "Epoch  5 | Train Loss: 239.804 | Train Acc: 45.89% | Train Prec: 44.04% | Train Recall: 40.56% | Train F1: 34.97% || Test Acc: 38.96% | Test Prec: 45.29% | Test Recall: 35.87% | Test F1: 30.34%\n",
      "Epoch  6 | Train Loss: 237.004 | Train Acc: 41.47% | Train Prec: 35.83% | Train Recall: 37.39% | Train F1: 31.07% || Test Acc: 36.91% | Test Prec: 31.60% | Test Recall: 33.41% | Test F1: 27.05%\n",
      "Epoch  7 | Train Loss: 234.250 | Train Acc: 42.38% | Train Prec: 31.75% | Train Recall: 36.88% | Train F1: 30.69% || Test Acc: 39.27% | Test Prec: 28.11% | Test Recall: 35.29% | Test F1: 28.12%\n",
      "Epoch  8 | Train Loss: 230.055 | Train Acc: 48.62% | Train Prec: 37.74% | Train Recall: 42.58% | Train F1: 35.69% || Test Acc: 41.64% | Test Prec: 36.38% | Test Recall: 38.25% | Test F1: 31.77%\n",
      "Epoch  9 | Train Loss: 228.135 | Train Acc: 46.05% | Train Prec: 29.48% | Train Recall: 40.42% | Train F1: 33.24% || Test Acc: 38.33% | Test Prec: 24.43% | Test Recall: 35.30% | Test F1: 28.18%\n",
      "Epoch 10 | Train Loss: 223.761 | Train Acc: 47.47% | Train Prec: 38.82% | Train Recall: 41.24% | Train F1: 33.92% || Test Acc: 43.53% | Test Prec: 49.76% | Test Recall: 39.76% | Test F1: 31.52%\n",
      "Epoch 11 | Train Loss: 220.915 | Train Acc: 40.72% | Train Prec: 28.33% | Train Recall: 35.12% | Train F1: 29.14% || Test Acc: 35.17% | Test Prec: 25.03% | Test Recall: 31.58% | Test F1: 25.66%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 1) Prepare dataset & split\n",
    "dataset = AccentSpectrogramDataset(\n",
    "    \"/Users/larsheijnen/DL/Train\",\n",
    "    target_sr=16000,\n",
    "    use_mel=True,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    n_mels=64,\n",
    "    log_scale=True)\n",
    "    \n",
    "train_len = int(0.8 * len(dataset))\n",
    "test_len  = len(dataset) - train_len\n",
    "train_ds, test_ds = random_split(dataset, [train_len, test_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,\n",
    "                          collate_fn=pad_collate)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
    "                          collate_fn=pad_collate)\n",
    "\n",
    "\n",
    "# 2) Model, criterion, optimizer with weight_decay\n",
    "device    = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3) Helper to evaluate on any loader\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for specs, labels in loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            outputs = model(specs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            \n",
    "    acc    = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1     = f1_score(all_labels, all_preds, average='macro')\n",
    "    return acc, prec, recall, f1\n",
    "\n",
    "\n",
    "for model_name, model_class in models_dict.items():\n",
    "    model = model_class().to(device)\n",
    "    print(f\"\\n=== Training model: {type(model).__name__} ===\")\n",
    "    \n",
    "    # apply weight decay or not\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    \n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for specs, labels in train_loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(specs), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # compute metrics\n",
    "        train_acc, train_prec, train_recall, train_f1 = evaluate(train_loader)\n",
    "        test_acc,  test_prec,  test_recall,  test_f1  = evaluate(test_loader)\n",
    "\n",
    "    \n",
    "        print(\n",
    "            f\"Epoch {epoch+1:2d} | \"\n",
    "            f\"Train Loss: {running_loss:.3f} | \"\n",
    "            f\"Train Acc: {train_acc*100:5.2f}% | \"\n",
    "            f\"Train Prec: {train_prec*100:5.2f}% | \"\n",
    "            f\"Train Recall: {train_recall*100:5.2f}% | \"\n",
    "            f\"Train F1: {train_f1*100:5.2f}% || \"\n",
    "            f\"Test Acc: {test_acc*100:5.2f}% | \"\n",
    "            f\"Test Prec: {test_prec*100:5.2f}% | \"\n",
    "            f\"Test Recall: {test_recall*100:5.2f}% | \"\n",
    "            f\"Test F1: {test_f1*100:5.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ik snap alleen ff niet welke van de 5 models hij hier nu pakt, neem aan dat we dit alleen op t beste model gaan doen\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_with_report(loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for specs, labels in loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            outputs = model(specs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, digits=3))\n",
    "\n",
    "print(\"Classification Report (Test Set):\")\n",
    "evaluate_with_report(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hier ook zo'n zelfde soort ding als net voor accenten maar dan voor gender\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier de code om het beste model toe te passen op de \"echt\" test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
