{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AccentSpectrogramDataset(Dataset):\n",
    "    def __init__(self, folder_path,\n",
    "                 target_sr: int = 16000,\n",
    "                 use_mel: bool = False,\n",
    "                 n_fft: int = 400,\n",
    "                 hop_length: int = None,\n",
    "                 n_mels: int = 64,\n",
    "                 log_scale: bool = True):\n",
    "        # store file paths only; transform per item\n",
    "        self.file_paths = [\n",
    "            os.path.join(folder_path, f)\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.endswith('.wav')\n",
    "        ]\n",
    "        self.target_sr = target_sr\n",
    "        self.use_mel = use_mel\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length or n_fft // 2\n",
    "        self.n_mels = n_mels\n",
    "        self.log_scale = log_scale\n",
    "\n",
    "        # pre-configure transform funct\n",
    "        if self.use_mel:\n",
    "            self._transform = lambda w: T.MelSpectrogram(\n",
    "                sample_rate=self.target_sr,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length,\n",
    "                n_mels=self.n_mels\n",
    "            )(w)\n",
    "        else:\n",
    "            self._transform = lambda w: T.Spectrogram(\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length\n",
    "            )(w)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        if sr != self.target_sr:\n",
    "            waveform = T.Resample(sr, self.target_sr)(waveform)\n",
    "\n",
    "        spec = self._transform(waveform)\n",
    "        if self.log_scale:\n",
    "            spec = torch.log(spec + 1e-6)\n",
    "\n",
    "        fname = os.path.basename(path)\n",
    "        accent = int(fname[0]) - 1          # classes 0–4\n",
    "        gender = fname[1]  # 'm' or 'f' \n",
    "        return spec, accent, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# def pad_collate(batch):\n",
    "#     specs, accents = zip(*batch)\n",
    "\n",
    "#     max_len = max([s.shape[-1] for s in specs])\n",
    "#     padded_specs = []\n",
    "\n",
    "#     for s in specs:\n",
    "#         pad_amount = max_len - s.shape[-1]\n",
    "#         padded = F.pad(s, (0, pad_amount))\n",
    "#         padded_specs.append(padded)\n",
    "\n",
    "    # return (\n",
    "#         torch.stack(padded_specs),             # [B, 1, Freq, Time]\n",
    "#         torch.tensor(accents),                # [B]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #baseline\n",
    "import torch.nn.functional as F\n",
    "def pad_collate(batch, target_width=208):\n",
    "    specs, accents, genders = zip(*batch)\n",
    "    padded_specs = []\n",
    "    for s in specs:\n",
    "        pad_amount = target_width - s.shape[-1]\n",
    "        if pad_amount > 0:\n",
    "            padded = torch.nn.functional.pad(s, (0, pad_amount))\n",
    "        else:\n",
    "            padded = s[..., :target_width]\n",
    "        padded_specs.append(padded)\n",
    "    return (\n",
    "        torch.stack(padded_specs),\n",
    "        torch.tensor(accents),\n",
    "        list(genders)   # <--- returns a list of 'm'/'f'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3166\n",
      "Spectrogram shape: torch.Size([1, 201, 526])\n",
      "Label: 1\n",
      "Gender: m\n"
     ]
    }
   ],
   "source": [
    "#dataset = AccentSpectrogramDataset(\"/Users/larsheijnen/DL/Train\")\n",
    "dataset = AccentSpectrogramDataset(\"/Users/larsheijnen/DL/Train\")\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "\n",
    "# Look at shape of first spectrogram\n",
    "x, y, z= dataset[6]\n",
    "print(f\"Spectrogram shape: {x.shape}\")\n",
    "print(f\"Label: {y}\")\n",
    "print(f\"Gender: {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrograms: torch.Size([4, 1, 201, 208])\n",
      "Accents: tensor([3, 2, 3, 2])\n",
      "Gender: ['f', 'f', 'm', 'm']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Use batch_size=4 for low RAM, pin_memory is False for macOS/MPS\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=pad_collate, pin_memory=False)\n",
    "\n",
    "# Try again\n",
    "for batch in dataloader:\n",
    "    spectrograms, accents, gender = batch\n",
    "    print(f\"Spectrograms: {spectrograms.shape}\")  # (B, 1, F, T)\n",
    "    print(f\"Accents: {accents}\")                  # (B,)\n",
    "    print(f\"Gender: {gender}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Model 1 (baseline)\n",
    "class CNNBaseline(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "    \n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "#Model 2 (baseline + batch normalization)\n",
    "class CNNBaseline_BatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "#Model 3 (baseline + dropout 0.3)\n",
    "class CNNBaseline_Dropout3(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "#Model 4 (baseline + dropout 0.5)\n",
    "class CNNBaseline_Dropout5(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.dropout = nn.Dropout(dropout_p) \n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "#Model 5 (baseline + bacth normalization + dropout 0.3)\n",
    "class CNNBaseline_Dropout3_BatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.dropout = nn.Dropout(dropout_p) \n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "#Model 6 (baseline + bacth normalization + dropout 0.5)\n",
    "class CNNBaseline_Dropout5_BatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.dropout = nn.Dropout(dropout_p) \n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AccentCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(8)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(16)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(32)\n",
    "        self.pool3 = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.fc = nn.Linear(32, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)               # → (B, 32)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"Model1\": CNNBaseline,\n",
    "    \"Model2\": CNNBaseline_BatchNorm, \n",
    "    \"Model3\": CNNBaseline_Dropout3,\n",
    "    \"Model4\": CNNBaseline_Dropout5,\n",
    "    \"Model5\": CNNBaseline_Dropout3_BatchNorm,\n",
    "    \"Model6\": CNNBaseline_Dropout5_BatchNorm,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training model: CNNBaseline ===\n",
      "Epoch  1 | Train Loss: 962.754 | Train Acc: 52.76% | Train Prec: 55.75% | Train Recall: 50.26% | Train F1: 48.43% || Test Acc: 46.85% | Test Prec: 48.52% | Test Recall: 45.59% | Test F1: 42.76%\n",
      "Epoch  2 | Train Loss: 508.916 | Train Acc: 80.81% | Train Prec: 83.60% | Train Recall: 76.96% | Train F1: 75.87% || Test Acc: 69.09% | Test Prec: 71.62% | Test Recall: 66.52% | Test F1: 64.08%\n",
      "Epoch  3 | Train Loss: 253.277 | Train Acc: 91.31% | Train Prec: 92.13% | Train Recall: 90.22% | Train F1: 90.82% || Test Acc: 78.55% | Test Prec: 81.74% | Test Recall: 77.01% | Test F1: 77.97%\n",
      "Epoch  4 | Train Loss: 143.240 | Train Acc: 89.73% | Train Prec: 90.32% | Train Recall: 90.12% | Train F1: 89.59% || Test Acc: 77.60% | Test Prec: 79.51% | Test Recall: 77.89% | Test F1: 77.30%\n",
      "Epoch  5 | Train Loss: 96.162 | Train Acc: 95.38% | Train Prec: 95.47% | Train Recall: 94.50% | Train F1: 94.70% || Test Acc: 79.97% | Test Prec: 81.20% | Test Recall: 78.78% | Test F1: 78.50%\n",
      "Epoch  6 | Train Loss: 70.708 | Train Acc: 98.14% | Train Prec: 97.86% | Train Recall: 98.18% | Train F1: 97.99% || Test Acc: 85.02% | Test Prec: 85.27% | Test Recall: 84.84% | Test F1: 84.63%\n",
      "Epoch  7 | Train Loss: 44.692 | Train Acc: 99.33% | Train Prec: 99.22% | Train Recall: 99.27% | Train F1: 99.24% || Test Acc: 85.96% | Test Prec: 85.74% | Test Recall: 85.54% | Test F1: 85.54%\n",
      "Epoch  8 | Train Loss: 30.131 | Train Acc: 99.72% | Train Prec: 99.66% | Train Recall: 99.73% | Train F1: 99.69% || Test Acc: 85.17% | Test Prec: 85.07% | Test Recall: 85.03% | Test F1: 84.91%\n",
      "Epoch  9 | Train Loss: 31.665 | Train Acc: 99.09% | Train Prec: 99.03% | Train Recall: 99.02% | Train F1: 99.02% || Test Acc: 82.02% | Test Prec: 81.60% | Test Recall: 81.03% | Test F1: 80.99%\n",
      "Epoch 10 | Train Loss: 21.880 | Train Acc: 99.37% | Train Prec: 99.19% | Train Recall: 99.40% | Train F1: 99.29% || Test Acc: 84.38% | Test Prec: 84.67% | Test Recall: 84.27% | Test F1: 84.25%\n",
      "\n",
      "Classification Report for CNNBaseline:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.906     0.936       138\n",
      "           1      0.888     0.805     0.844       118\n",
      "           2      0.866     0.858     0.862       120\n",
      "           3      0.837     0.820     0.828       150\n",
      "           4      0.674     0.824     0.742       108\n",
      "\n",
      "    accuracy                          0.844       634\n",
      "   macro avg      0.847     0.843     0.843       634\n",
      "weighted avg      0.853     0.844     0.846       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline:\n",
      "Male: {'accuracy': 0.8280254777070064, 'precision': 0.8392922366056694, 'recall': 0.8304690859473102, 'f1': 0.831988898123352}\n",
      "Female: {'accuracy': 0.859375, 'precision': 0.8509690971576547, 'recall': 0.8504559090894123, 'f1': 0.8490389401258966}\n",
      "\n",
      "=== Training model: CNNBaseline_BatchNorm ===\n",
      "Epoch  1 | Train Loss: 900.053 | Train Acc: 57.42% | Train Prec: 61.93% | Train Recall: 56.58% | Train F1: 54.04% || Test Acc: 51.42% | Test Prec: 56.48% | Test Recall: 50.26% | Test F1: 48.06%\n",
      "Epoch  2 | Train Loss: 482.003 | Train Acc: 88.35% | Train Prec: 87.64% | Train Recall: 88.18% | Train F1: 87.63% || Test Acc: 77.92% | Test Prec: 78.39% | Test Recall: 78.02% | Test F1: 77.59%\n",
      "Epoch  3 | Train Loss: 263.450 | Train Acc: 92.46% | Train Prec: 92.82% | Train Recall: 91.55% | Train F1: 91.98% || Test Acc: 79.02% | Test Prec: 81.04% | Test Recall: 77.64% | Test F1: 78.40%\n",
      "Epoch  4 | Train Loss: 169.465 | Train Acc: 94.75% | Train Prec: 95.18% | Train Recall: 93.84% | Train F1: 94.36% || Test Acc: 81.70% | Test Prec: 83.58% | Test Recall: 80.47% | Test F1: 81.06%\n",
      "Epoch  5 | Train Loss: 111.042 | Train Acc: 94.98% | Train Prec: 95.97% | Train Recall: 94.02% | Train F1: 94.72% || Test Acc: 78.23% | Test Prec: 81.61% | Test Recall: 76.66% | Test F1: 77.76%\n",
      "Epoch  6 | Train Loss: 83.350 | Train Acc: 97.00% | Train Prec: 96.69% | Train Recall: 96.90% | Train F1: 96.75% || Test Acc: 82.65% | Test Prec: 82.20% | Test Recall: 82.59% | Test F1: 82.12%\n",
      "Epoch  7 | Train Loss: 52.118 | Train Acc: 99.33% | Train Prec: 99.22% | Train Recall: 99.33% | Train F1: 99.27% || Test Acc: 84.23% | Test Prec: 84.38% | Test Recall: 83.50% | Test F1: 83.84%\n",
      "Epoch  8 | Train Loss: 45.238 | Train Acc: 99.21% | Train Prec: 99.21% | Train Recall: 99.11% | Train F1: 99.15% || Test Acc: 86.12% | Test Prec: 86.21% | Test Recall: 85.61% | Test F1: 85.79%\n",
      "Epoch  9 | Train Loss: 29.368 | Train Acc: 99.33% | Train Prec: 99.17% | Train Recall: 99.39% | Train F1: 99.27% || Test Acc: 85.49% | Test Prec: 85.09% | Test Recall: 85.26% | Test F1: 85.13%\n",
      "Epoch 10 | Train Loss: 42.260 | Train Acc: 99.33% | Train Prec: 99.22% | Train Recall: 99.31% | Train F1: 99.26% || Test Acc: 83.60% | Test Prec: 83.76% | Test Recall: 83.28% | Test F1: 83.39%\n",
      "\n",
      "Classification Report for CNNBaseline_BatchNorm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.985     0.949     0.967       138\n",
      "           1      0.888     0.873     0.880       118\n",
      "           2      0.860     0.767     0.811       120\n",
      "           3      0.786     0.807     0.796       150\n",
      "           4      0.669     0.769     0.716       108\n",
      "\n",
      "    accuracy                          0.836       634\n",
      "   macro avg      0.838     0.833     0.834       634\n",
      "weighted avg      0.842     0.836     0.838       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline_BatchNorm:\n",
      "Male: {'accuracy': 0.8089171974522293, 'precision': 0.8200327110206687, 'recall': 0.809629861963205, 'f1': 0.8133692643414457}\n",
      "Female: {'accuracy': 0.8625, 'precision': 0.8546774190645158, 'recall': 0.842442320939474, 'f1': 0.8456685196411223}\n",
      "\n",
      "=== Training model: CNNBaseline_Dropout3 ===\n",
      "Epoch  1 | Train Loss: 814.741 | Train Acc: 66.47% | Train Prec: 71.63% | Train Recall: 63.22% | Train F1: 63.95% || Test Acc: 57.89% | Test Prec: 64.42% | Test Recall: 54.94% | Test F1: 54.94%\n",
      "Epoch  2 | Train Loss: 360.740 | Train Acc: 88.27% | Train Prec: 87.99% | Train Recall: 87.84% | Train F1: 87.50% || Test Acc: 76.18% | Test Prec: 78.07% | Test Recall: 75.33% | Test F1: 75.34%\n",
      "Epoch  3 | Train Loss: 213.028 | Train Acc: 91.15% | Train Prec: 91.37% | Train Recall: 89.96% | Train F1: 90.36% || Test Acc: 84.86% | Test Prec: 85.89% | Test Recall: 84.03% | Test F1: 84.54%\n",
      "Epoch  4 | Train Loss: 161.566 | Train Acc: 93.80% | Train Prec: 94.52% | Train Recall: 92.66% | Train F1: 93.32% || Test Acc: 82.65% | Test Prec: 84.04% | Test Recall: 81.55% | Test F1: 82.18%\n",
      "Epoch  5 | Train Loss: 109.770 | Train Acc: 92.18% | Train Prec: 91.89% | Train Recall: 92.73% | Train F1: 92.01% || Test Acc: 81.86% | Test Prec: 82.34% | Test Recall: 82.15% | Test F1: 81.63%\n",
      "Epoch  6 | Train Loss: 85.746 | Train Acc: 98.42% | Train Prec: 98.17% | Train Recall: 98.46% | Train F1: 98.30% || Test Acc: 86.75% | Test Prec: 86.35% | Test Recall: 86.67% | Test F1: 86.41%\n",
      "Epoch  7 | Train Loss: 45.923 | Train Acc: 96.88% | Train Prec: 96.78% | Train Recall: 96.70% | Train F1: 96.65% || Test Acc: 82.33% | Test Prec: 85.18% | Test Recall: 81.45% | Test F1: 82.21%\n",
      "Epoch  8 | Train Loss: 51.083 | Train Acc: 96.84% | Train Prec: 96.60% | Train Recall: 96.82% | Train F1: 96.57% || Test Acc: 82.18% | Test Prec: 84.18% | Test Recall: 81.52% | Test F1: 81.92%\n",
      "Epoch  9 | Train Loss: 32.339 | Train Acc: 99.68% | Train Prec: 99.70% | Train Recall: 99.59% | Train F1: 99.64% || Test Acc: 88.80% | Test Prec: 89.01% | Test Recall: 88.23% | Test F1: 88.38%\n",
      "Epoch 10 | Train Loss: 43.942 | Train Acc: 99.09% | Train Prec: 99.02% | Train Recall: 99.03% | Train F1: 99.03% || Test Acc: 85.96% | Test Prec: 86.38% | Test Recall: 85.46% | Test F1: 85.82%\n",
      "\n",
      "Classification Report for CNNBaseline_Dropout3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.942     0.949     0.946       138\n",
      "           1      0.945     0.873     0.907       118\n",
      "           2      0.872     0.850     0.861       120\n",
      "           3      0.768     0.860     0.811       150\n",
      "           4      0.792     0.741     0.766       108\n",
      "\n",
      "    accuracy                          0.860       634\n",
      "   macro avg      0.864     0.855     0.858       634\n",
      "weighted avg      0.863     0.860     0.860       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline_Dropout3:\n",
      "Male: {'accuracy': 0.856687898089172, 'precision': 0.8646137201471513, 'recall': 0.8626281228822963, 'f1': 0.8612923050632671}\n",
      "Female: {'accuracy': 0.8625, 'precision': 0.857657000376817, 'recall': 0.8483729979939149, 'f1': 0.8501324316378973}\n",
      "\n",
      "=== Training model: CNNBaseline_Dropout5 ===\n",
      "Epoch  1 | Train Loss: 834.992 | Train Acc: 73.06% | Train Prec: 72.94% | Train Recall: 71.38% | Train F1: 71.42% || Test Acc: 67.67% | Test Prec: 68.80% | Test Recall: 67.19% | Test F1: 67.16%\n",
      "Epoch  2 | Train Loss: 350.444 | Train Acc: 87.48% | Train Prec: 87.14% | Train Recall: 86.08% | Train F1: 86.32% || Test Acc: 79.50% | Test Prec: 79.41% | Test Recall: 79.07% | Test F1: 79.03%\n",
      "Epoch  3 | Train Loss: 209.705 | Train Acc: 95.46% | Train Prec: 95.67% | Train Recall: 94.54% | Train F1: 94.96% || Test Acc: 84.38% | Test Prec: 85.33% | Test Recall: 83.65% | Test F1: 84.12%\n",
      "Epoch  4 | Train Loss: 129.607 | Train Acc: 97.31% | Train Prec: 97.15% | Train Recall: 96.85% | Train F1: 96.96% || Test Acc: 84.86% | Test Prec: 85.31% | Test Recall: 84.22% | Test F1: 84.45%\n",
      "Epoch  5 | Train Loss: 86.640 | Train Acc: 98.30% | Train Prec: 98.48% | Train Recall: 98.07% | Train F1: 98.25% || Test Acc: 85.17% | Test Prec: 86.50% | Test Recall: 84.56% | Test F1: 85.23%\n",
      "Epoch  6 | Train Loss: 55.257 | Train Acc: 96.80% | Train Prec: 97.42% | Train Recall: 95.94% | Train F1: 96.51% || Test Acc: 84.54% | Test Prec: 86.30% | Test Recall: 83.34% | Test F1: 83.89%\n",
      "Epoch  7 | Train Loss: 48.499 | Train Acc: 99.37% | Train Prec: 99.20% | Train Recall: 99.38% | Train F1: 99.29% || Test Acc: 86.59% | Test Prec: 86.84% | Test Recall: 86.46% | Test F1: 86.43%\n",
      "Epoch  8 | Train Loss: 26.044 | Train Acc: 99.68% | Train Prec: 99.67% | Train Recall: 99.64% | Train F1: 99.66% || Test Acc: 87.07% | Test Prec: 86.93% | Test Recall: 86.77% | Test F1: 86.80%\n",
      "Epoch  9 | Train Loss: 38.862 | Train Acc: 98.38% | Train Prec: 98.55% | Train Recall: 98.05% | Train F1: 98.28% || Test Acc: 83.44% | Test Prec: 84.29% | Test Recall: 82.26% | Test F1: 82.60%\n",
      "Epoch 10 | Train Loss: 44.776 | Train Acc: 99.25% | Train Prec: 99.22% | Train Recall: 99.20% | Train F1: 99.21% || Test Acc: 85.96% | Test Prec: 86.53% | Test Recall: 85.29% | Test F1: 85.73%\n",
      "\n",
      "Classification Report for CNNBaseline_Dropout5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.985     0.949     0.967       138\n",
      "           1      0.952     0.847     0.897       118\n",
      "           2      0.861     0.875     0.868       120\n",
      "           3      0.759     0.880     0.815       150\n",
      "           4      0.770     0.713     0.740       108\n",
      "\n",
      "    accuracy                          0.860       634\n",
      "   macro avg      0.865     0.853     0.857       634\n",
      "weighted avg      0.865     0.860     0.861       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline_Dropout5:\n",
      "Male: {'accuracy': 0.8312101910828026, 'precision': 0.8440915752163489, 'recall': 0.8285824789734495, 'f1': 0.8335767129789791}\n",
      "Female: {'accuracy': 0.8875, 'precision': 0.8847877511905612, 'recall': 0.8708847849782823, 'f1': 0.874888798028237}\n",
      "\n",
      "=== Training model: CNNBaseline_Dropout3_BatchNorm ===\n",
      "Epoch  1 | Train Loss: 933.762 | Train Acc: 57.74% | Train Prec: 59.10% | Train Recall: 55.37% | Train F1: 54.26% || Test Acc: 51.10% | Test Prec: 54.07% | Test Recall: 49.46% | Test F1: 48.75%\n",
      "Epoch  2 | Train Loss: 468.326 | Train Acc: 88.82% | Train Prec: 88.86% | Train Recall: 87.35% | Train F1: 87.70% || Test Acc: 80.91% | Test Prec: 81.49% | Test Recall: 79.84% | Test F1: 79.88%\n",
      "Epoch  3 | Train Loss: 215.874 | Train Acc: 90.40% | Train Prec: 90.96% | Train Recall: 88.95% | Train F1: 89.41% || Test Acc: 80.28% | Test Prec: 81.58% | Test Recall: 79.07% | Test F1: 79.14%\n",
      "Epoch  4 | Train Loss: 124.551 | Train Acc: 97.08% | Train Prec: 97.15% | Train Recall: 96.52% | Train F1: 96.78% || Test Acc: 87.07% | Test Prec: 87.38% | Test Recall: 86.45% | Test F1: 86.69%\n",
      "Epoch  5 | Train Loss: 83.116 | Train Acc: 96.84% | Train Prec: 96.78% | Train Recall: 96.48% | Train F1: 96.58% || Test Acc: 83.44% | Test Prec: 83.85% | Test Recall: 82.51% | Test F1: 82.52%\n",
      "Epoch  6 | Train Loss: 56.762 | Train Acc: 99.09% | Train Prec: 98.95% | Train Recall: 99.10% | Train F1: 99.02% || Test Acc: 86.75% | Test Prec: 87.06% | Test Recall: 86.41% | Test F1: 86.66%\n",
      "Epoch  7 | Train Loss: 45.491 | Train Acc: 95.93% | Train Prec: 95.86% | Train Recall: 95.65% | Train F1: 95.58% || Test Acc: 84.23% | Test Prec: 85.29% | Test Recall: 83.13% | Test F1: 83.36%\n",
      "Epoch  8 | Train Loss: 34.966 | Train Acc: 97.83% | Train Prec: 97.69% | Train Recall: 97.68% | Train F1: 97.63% || Test Acc: 86.91% | Test Prec: 87.38% | Test Recall: 86.86% | Test F1: 86.70%\n",
      "Epoch  9 | Train Loss: 32.039 | Train Acc: 99.53% | Train Prec: 99.44% | Train Recall: 99.52% | Train F1: 99.47% || Test Acc: 87.38% | Test Prec: 87.45% | Test Recall: 87.24% | Test F1: 87.31%\n",
      "Epoch 10 | Train Loss: 28.922 | Train Acc: 99.64% | Train Prec: 99.66% | Train Recall: 99.55% | Train F1: 99.60% || Test Acc: 88.80% | Test Prec: 88.75% | Test Recall: 88.44% | Test F1: 88.54%\n",
      "\n",
      "Classification Report for CNNBaseline_Dropout3_BatchNorm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.977     0.942     0.959       138\n",
      "           1      0.902     0.932     0.917       118\n",
      "           2      0.908     0.908     0.908       120\n",
      "           3      0.830     0.880     0.854       150\n",
      "           4      0.820     0.759     0.788       108\n",
      "\n",
      "    accuracy                          0.888       634\n",
      "   macro avg      0.888     0.884     0.885       634\n",
      "weighted avg      0.889     0.888     0.888       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline_Dropout3_BatchNorm:\n",
      "Male: {'accuracy': 0.8757961783439491, 'precision': 0.8816068073421015, 'recall': 0.8746169517399462, 'f1': 0.8774843795766977}\n",
      "Female: {'accuracy': 0.9, 'precision': 0.8940094871190082, 'recall': 0.8885955099739936, 'f1': 0.8902276180422678}\n",
      "\n",
      "=== Training model: CNNBaseline_Dropout5_BatchNorm ===\n",
      "Epoch  1 | Train Loss: 761.585 | Train Acc: 84.16% | Train Prec: 83.58% | Train Recall: 82.59% | Train F1: 82.51% || Test Acc: 76.18% | Test Prec: 75.57% | Test Recall: 75.27% | Test F1: 74.47%\n",
      "Epoch  2 | Train Loss: 307.960 | Train Acc: 90.56% | Train Prec: 90.99% | Train Recall: 88.65% | Train F1: 89.14% || Test Acc: 80.60% | Test Prec: 80.99% | Test Recall: 79.09% | Test F1: 78.85%\n",
      "Epoch  3 | Train Loss: 225.140 | Train Acc: 92.89% | Train Prec: 92.61% | Train Recall: 92.54% | Train F1: 92.25% || Test Acc: 82.02% | Test Prec: 83.01% | Test Recall: 81.67% | Test F1: 81.71%\n",
      "Epoch  4 | Train Loss: 138.836 | Train Acc: 93.25% | Train Prec: 94.07% | Train Recall: 91.30% | Train F1: 91.67% || Test Acc: 79.97% | Test Prec: 80.69% | Test Recall: 78.31% | Test F1: 76.98%\n",
      "Epoch  5 | Train Loss: 106.011 | Train Acc: 88.15% | Train Prec: 91.18% | Train Recall: 85.57% | Train F1: 85.39% || Test Acc: 76.66% | Test Prec: 79.06% | Test Recall: 75.05% | Test F1: 72.39%\n",
      "Epoch  6 | Train Loss: 72.907 | Train Acc: 94.87% | Train Prec: 94.69% | Train Recall: 94.65% | Train F1: 94.27% || Test Acc: 81.23% | Test Prec: 83.78% | Test Recall: 80.34% | Test F1: 80.51%\n",
      "Epoch  7 | Train Loss: 53.586 | Train Acc: 97.47% | Train Prec: 97.67% | Train Recall: 96.97% | Train F1: 97.28% || Test Acc: 83.28% | Test Prec: 84.36% | Test Recall: 82.10% | Test F1: 82.66%\n",
      "Epoch  8 | Train Loss: 54.650 | Train Acc: 98.06% | Train Prec: 98.24% | Train Recall: 97.61% | Train F1: 97.88% || Test Acc: 81.55% | Test Prec: 82.76% | Test Recall: 80.34% | Test F1: 80.61%\n",
      "Epoch  9 | Train Loss: 21.664 | Train Acc: 98.42% | Train Prec: 98.13% | Train Recall: 98.40% | Train F1: 98.22% || Test Acc: 85.96% | Test Prec: 87.24% | Test Recall: 85.40% | Test F1: 85.83%\n",
      "Epoch 10 | Train Loss: 46.512 | Train Acc: 98.50% | Train Prec: 98.31% | Train Recall: 98.55% | Train F1: 98.41% || Test Acc: 84.54% | Test Prec: 84.67% | Test Recall: 84.30% | Test F1: 83.85%\n",
      "\n",
      "Classification Report for CNNBaseline_Dropout5_BatchNorm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.943     0.964     0.953       138\n",
      "           1      0.824     0.949     0.882       118\n",
      "           2      0.753     0.917     0.827       120\n",
      "           3      0.862     0.747     0.800       150\n",
      "           4      0.852     0.639     0.730       108\n",
      "\n",
      "    accuracy                          0.845       634\n",
      "   macro avg      0.847     0.843     0.839       634\n",
      "weighted avg      0.850     0.845     0.842       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline_Dropout5_BatchNorm:\n",
      "Male: {'accuracy': 0.8535031847133758, 'precision': 0.8513371228964111, 'recall': 0.8594405385714665, 'f1': 0.8523058400148183}\n",
      "Female: {'accuracy': 0.8375, 'precision': 0.8345416863645575, 'recall': 0.8261257906013674, 'f1': 0.8135926833291768}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Prepare dataset & split\n",
    "dataset = AccentSpectrogramDataset(\n",
    "    '/Users/larsheijnen/DL/Train',\n",
    "    target_sr=16000,\n",
    "    use_mel=True,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    n_mels=64,\n",
    "    log_scale=True)\n",
    "\n",
    "train_len = int(0.8 * len(dataset))\n",
    "test_len  = len(dataset) - train_len\n",
    "train_ds, test_ds = random_split(dataset, [train_len, test_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,  collate_fn=pad_collate)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "device    = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# General (not by gender) evaluation helper\n",
    "def evaluate(loader, model, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for specs, labels, _ in loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            outputs = model(specs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "    acc    = accuracy_score(all_labels, all_preds)\n",
    "    prec   = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1     = f1_score(all_labels, all_preds, average='macro')\n",
    "    return acc, prec, recall, f1\n",
    "\n",
    "# Gender-based evaluation helper\n",
    "def evaluate_by_gender(loader, model, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_genders = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for specs, labels, genders in loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            outputs = model(specs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            all_genders.extend(genders)\n",
    "    results = {}\n",
    "    for gender in ['m', 'f']:\n",
    "        idxs = [i for i, g in enumerate(all_genders) if g == gender]\n",
    "        gender_preds = [all_preds[i] for i in idxs]\n",
    "        gender_labels = [all_labels[i] for i in idxs]\n",
    "        acc = accuracy_score(gender_labels, gender_preds)\n",
    "        prec = precision_score(gender_labels, gender_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(gender_labels, gender_preds, average='macro')\n",
    "        f1 = f1_score(gender_labels, gender_preds, average='macro')\n",
    "        results[gender] = {'accuracy': acc, 'precision': prec, 'recall': recall, 'f1': f1}\n",
    "    return results\n",
    "\n",
    "def classification_report_for_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for specs, labels, _ in loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            outputs = model(specs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "    print(classification_report(all_labels, all_preds, digits=3))\n",
    "\n",
    "for model_name, model_class in models_dict.items():\n",
    "    model = model_class().to(device)\n",
    "    print(f\"\\n=== Training model: {type(model).__name__} ===\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for specs, labels, genders in train_loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(specs), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Compute and print general metrics for this epoch (not by gender)\n",
    "        train_acc, train_prec, train_recall, train_f1 = evaluate(train_loader, model, device)\n",
    "        test_acc, test_prec, test_recall, test_f1 = evaluate(test_loader, model, device)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:2d} | \"\n",
    "            f\"Train Loss: {running_loss:.3f} | \"\n",
    "            f\"Train Acc: {train_acc*100:5.2f}% | \"\n",
    "            f\"Train Prec: {train_prec*100:5.2f}% | \"\n",
    "            f\"Train Recall: {train_recall*100:5.2f}% | \"\n",
    "            f\"Train F1: {train_f1*100:5.2f}% || \"\n",
    "            f\"Test Acc: {test_acc*100:5.2f}% | \"\n",
    "            f\"Test Prec: {test_prec*100:5.2f}% | \"\n",
    "            f\"Test Recall: {test_recall*100:5.2f}% | \"\n",
    "            f\"Test F1: {test_f1*100:5.2f}%\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nClassification Report for {type(model).__name__}:\")\n",
    "    classification_report_for_model(model, test_loader, device)\n",
    "\n",
    "    print(f\"\\nGender breakdown for {type(model).__name__}:\")\n",
    "    gender_results = evaluate_by_gender(test_loader, model, device)\n",
    "    for gender in gender_results:\n",
    "        label = \"Male\" if gender == \"m\" else \"Female\"\n",
    "        print(f\"{label}: {gender_results[gender]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier de code om het beste model toe te passen op de \"echt\" test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
