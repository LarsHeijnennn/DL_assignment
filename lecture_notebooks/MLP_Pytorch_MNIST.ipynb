{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP in PyTorch on a subset of MNIST.\n",
    "In this notebook you will work on training an MLP on the MNIST dataset. Here, the module class is introducend and you will become familiar with a more flexible way of defining models in PyTorch than the sequential container.\n",
    "\n",
    "### MNIST dataset:\n",
    "The MNIST dataset is a widely used benchmark dataset in the field of machine learning, particularly in the domain of image classification. It consists of a collection of 28x28 pixel grayscale images depicting handwritten digits from 0 to 9. Originally created to train and test the performance of various machine learning algorithms, MNIST has become a standard dataset for assessing the capabilities of new models. In scikit-learn, the MNIST dataset is conveniently accessible through the fetch_openml function, where each image is flattened into a one-dimensional array of 784 elements (28x28 pixels). The associated labels represent the corresponding digit for each image, allowing for straightforward integration with various classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "In the code below, the data is loaded from scikit learn and a PyTorch dataloader is created on a subset of the data containing 1000 samples.\n",
    "\n",
    "The code starts by importing necessary libraries, including PyTorch, torchvision, and matplotlib.\n",
    "The MNIST dataset is loaded and transformed into PyTorch tensors using transforms.ToTensor().\n",
    "A subset of the dataset is created to speed up training. This can be adjusted based on available computational resources.\n",
    "Finally, a DataLoader is created for the training and testing sets. This is done for efficient batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the Optical Recognition of Handwritten Digits dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "full_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Subset the dataset for faster training (optional)\n",
    "subset_size = 15000\n",
    "subset, _ = random_split(full_dataset, [subset_size, len(full_dataset) - subset_size])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(0.8 * len(subset))\n",
    "test_size = len(subset) - train_size\n",
    "train_dataset, test_dataset = random_split(subset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create DataLoader for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization:\n",
    "Before we start working on the MLP and its training, we can visualize the data to get a better idea of the problem at hand. The code below addresses this. It a function visualize_dataset to visualize a random sample of the dataset. It then visualizes a small number of samples, displaying the handwritten digits along with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "# Function to visualize a random sample of the dataset\n",
    "def visualize_dataset(dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(12, 3))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "        image, label = dataset[sample_idx]\n",
    "        axes[i].imshow(image.squeeze(), cmap='gray')\n",
    "        axes[i].set_title(f'Label: {label}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a random sample of the dataset\n",
    "visualize_dataset(subset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "### The Module class: \n",
    "In PyTorch, the `nn.Module` class serves as a foundational building block for constructing and organizing neural network architectures. It provides a flexible and modular structure that allows users to define their custom models by subclassing `nn.Module`. The `nn.Module` class encapsulates both trainable parameters and the operations performed on input data, facilitating the design of complex and hierarchical models. Subclassing `nn.Module` requires implementing the `__init__` and `forward` methods, where the former initializes the model's components, and the latter defines the forward pass. This object-oriented approach offers a fine-grained level of control over the model's architecture, making it suitable for scenarios where more intricate designs and customization are necessary.\n",
    "\n",
    "\n",
    "### Sequential as a Subclass of nn.Module\n",
    "\n",
    "In PyTorch, `nn.Sequential` is a subclass of `nn.Module`, meaning it inherits all the functionalities of `nn.Module`, including parameter management, saving and loading models, and moving models to different devices (CPU/GPU). The key difference is that `nn.Sequential` provides an easy way to define models where the forward pass is simply a sequential execution of layers, without needing to explicitly define a `forward` method.\n",
    "\n",
    "The `torch.nn.Sequential` module allows for building neural networks in a concise and modular way. The `Sequential` container stacks layers in the order they are defined, automatically passing the output of one layer as the input to the next.\n",
    "\n",
    "\n",
    "#### Defining a Sequential Model\n",
    "\n",
    "A `Sequential` model can be created using `torch.nn.Sequential`, where layers are added in the order they should be executed:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple feedforward network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),  # Fully connected layer: input size 10, output size 20\n",
    "    nn.ReLU(),          # Activation function\n",
    "    nn.Linear(20, 5),   # Fully connected layer: input size 20, output size 5\n",
    ")\n",
    "\n",
    "print(model)`\n",
    "```\n",
    "\n",
    "### More general models \n",
    "Defining models by extending the `nn.Module` class in PyTorch can provide several advantages over using the `nn.Sequential` container. One key advantage is increased flexibility in designing complex architectures that involve branching, skipping connections, or any other non-sequential patterns. With the `nn.Module` approach, one can easily create models with shared parameters, reuse submodules, and incorporate conditional logic within the forward pass. This level of flexibility is particularly beneficial when dealing with intricate architectures common in advanced tasks such as image segmentation, object detection, or natural language processing. Additionally, subclassing `nn.Module` allows for better organization and encapsulation of model components, promoting modular and maintainable code structures. Overall, leveraging the `nn.Module` class offers a powerful and extensible framework for crafting sophisticated neural network architectures in PyTorch. We will focus on extending the `nn.Module` in later notebooks.\n",
    "\n",
    "\n",
    "**Code:**\n",
    "The below code defines an MLP model using the `nn.Module` class, with one hidden layer and a ReLU activation function using the `nn.Sequential`.\n",
    "The input size is set based on the size of MNIST images (28x28 pixels), the hidden size is set to 64, and the output size corresponds to the number of classes (digits 0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training cycle\n",
    "Here, your task is to define and implement the training loop and necessary components to train the model. The training loos is to be imlemented in afunction `train_model`. The function takes as inputs the model to be trained (MLP_model) and the number of epochs (num_epochs), which determines how many times the entire dataset will be passed through the network during training. \n",
    "\n",
    "Below is a breakdown of the steps you need to take:\n",
    "\n",
    "* Initialize the Loss Function and Optimizer:\n",
    "    * Use CrossEntropyLoss as the loss function.\n",
    "    * Use Stochastic Gradient Descent (SGD) as the optimizer.\n",
    "\n",
    "* Training Loop:\n",
    "\n",
    "    * Iterate over each epoch using for epoch in range(num_epochs): Initialize total_loss to 0 for each epoch.\n",
    "\n",
    "    * Inner Loop for Batches: Iterate over batches of data from the training DataLoader (train_loader).\n",
    "\n",
    "    * For each batch, perform the following steps: \n",
    "        * Zero the gradients using optimizer.zero_grad().\n",
    "        * Flatten the input images if needed (in this case, using inputs.view(-1, 28 * 28)). \n",
    "        * Perform a forward pass through the model (outputs = model(inputs)). Compute the loss between the model predictions and the true labels (loss = criterion(outputs, labels)).\n",
    "        * Perform a backward pass to compute gradients (loss.backward()).\n",
    "        * Update the model parameters using the optimizer (optimizer.step()).\n",
    "        * Accumulate the batch loss to the total loss.\n",
    "    \n",
    "    * Print Epoch Information: After each epoch, print the average loss over all batches in that epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Cycle\n",
    "\n",
    "def train_model(MLP_model, num_epochs = 10):\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(MLP_model.parameters(), lr=0.01)\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            inputs = inputs.view(-1, 28 * 28)  # Flatten the images\n",
    "            outputs = MLP_model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "After training, we need to evaluate the model's performance on the test set.\n",
    "Below, you need to implement the function `evaluate_model`.\n",
    "In this function you need to do the following:\n",
    "\n",
    "- **Set Model to Evaluation Mode:**\n",
    "  - Inside the function, set the provided MLP model (`MLP_model`) to evaluation mode using `MLP_model.eval()`.\n",
    "\n",
    "- **Perform Evaluation:**\n",
    "  - Use `torch.no_grad()` to disable gradient computation during evaluation, as gradients are not needed for this phase.\n",
    "  - Iterate through batches in the provided `test_loader`.\n",
    "  - Flatten the test images appropriately within each batch.\n",
    "  - Pass the flattened test images through the model (`MLP_model`).\n",
    "  - Use `torch.max` to obtain the predicted class for each test sample.\n",
    "  - Accumulate correct predictions and total samples over all batches.\n",
    "\n",
    "- **Print Accuracy:**\n",
    "  - Calculate the accuracy by dividing the total correct predictions by the total number of samples in the test set.\n",
    "  - Print the test accuracy to the console using `print(f'Test Accuracy: {accuracy * 100:.2f}%')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(MLP_model, test_loader):\n",
    "    # Model Evaluation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        MLP_model.eval()  # Set the model to evaluation mode\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(-1, 28 * 28)  # Flatten the test images\n",
    "            outputs = MLP_model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together.\n",
    "The code below initializes an MLP model, trains it on the training dataset for the given number of epochs (10 in this case) using the train_model function, and then evaluates its performance on the test dataset using the evaluate_model function. You should get a performance of around $~89.5%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "input_size = 28 * 28  # MNIST images are 28x28 pixels\n",
    "hidden_size = 64\n",
    "output_size = 10  # 10 classes for digits 0 to 9\n",
    "# Create the model\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "# Train the model\n",
    "train_model(model, num_epochs=10)\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with the MLP.\n",
    "\n",
    "Now you can start experimenting and changing the MLP architecture and see if that affects the performance of the model.\n",
    "Implement a ModifiedMLP class that addresses the following:\n",
    "* **1. Change the activation function in the hidden layers:** The constructor should recieves as input the activation function to be used in the hidden layers.\n",
    "* **2. Change the number of hidden layers and their sizes:** The constructor should recieve as input a list with the sizes of all the hidden layers in the MLP.\n",
    "\n",
    "To achieve this, it is a good idea to use the `nn.ModuleList` class.\n",
    "\n",
    "### Understanding `nn.ModuleList` in PyTorch\n",
    "\n",
    "#### What is `nn.ModuleList`?\n",
    "In PyTorch, `nn.ModuleList` is a container module that holds a list of `nn.Module` instances. It allows you to manage a sequence of modules within a PyTorch model. When you use `nn.ModuleList`, the contained modules become sub-modules of the parent module, and PyTorch can track and manage their parameters.\n",
    "\n",
    "#### How Does `nn.ModuleList` Work?\n",
    "1. **Initialization:**\n",
    "   - `nn.ModuleList` is initialized with a list of `nn.Module` instances.\n",
    "2. **Usage in the Context of our ModifiedMLP Class:**\n",
    "   - In the `ModifiedMLP` class, `nn.ModuleList` can be utilized to store the linear layers and activation functions dynamically created during initialization. Layers can be added using the append function as any list in python.\n",
    "3. **Iteration:**\n",
    "   - You can iterate over the modules within the `nn.ModuleList` during the forward pass or other operations.\n",
    "\n",
    "#### Why Use `nn.ModuleList` in This Context?\n",
    "- **Dynamic Architecture:**\n",
    "  - In our `ModifiedMLP` class, for example, the number of hidden layers and their sizes can vary based on user input.\n",
    "  - Using `nn.ModuleList` allows for dynamic creation and storage of these layers.\n",
    "\n",
    "\n",
    "#### How to Use `nn.ModuleList` in our `ModifiedMLP`:\n",
    "1. **Initialization:**\n",
    "   - `self.layers = nn.ModuleList()` initializes an empty `nn.ModuleList`.\n",
    "2. **Dynamic Creation of Layers:**\n",
    "   - Inside the loop, linear layers and activation functions are dynamically created and added to `self.layers` with the `self.layers.append()` function. A linear layer (an instance of nn.Linear) is created and added to self.layers. For the activations function, an instance of `activation_function` is created and added to the `nn.ModuleList` object.\n",
    "3. **Access and Iteration:**\n",
    "   - During the forward pass (`forward` method), layers can be accessed and iterated through using `for layer in self.layers:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModifiedMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation_function):\n",
    "        \"\"\"\n",
    "        Initialize the MLP model.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size (int): Size of the input features.\n",
    "        - hidden_sizes (list): List containing the sizes of hidden layers.\n",
    "        - output_size (int): Size of the output layer.\n",
    "        - activation_function (torch.nn.Module): Activation function for hidden layers.\n",
    "        \"\"\"\n",
    "        super(ModifiedMLP, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Create hidden layers and activations dynamically\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            # Linear layer\n",
    "            self.layers.append(nn.Linear(input_size if i == 0 else hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "\n",
    "            # Activation function (except for the last layer)\n",
    "            self.layers.append(activation_function())\n",
    "\n",
    "        # Append the ouptu layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], self.output_size))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the MLP model.\n",
    "\n",
    "        Parameters:\n",
    "        - x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        # Flatten the input\n",
    "        x = x.view(-1, self.input_size)\n",
    "\n",
    "        # Forward pass through hidden layers with activation functions\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the ModifiedMLP class is implemented we can start experimenting.\n",
    "First, let's try using the Sigmoid activation function with two hidden layers of size 64.\n",
    "Train the model and compare its performance with the model you trained above. Is there a difference? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "input_size = 28 * 28  # MNIST images are 28x28 pixels\n",
    "output_size = 10  # 10 classes for digits 0 to 9\n",
    "activation_function = nn.Sigmoid\n",
    "hidden_sizes = [64, 64]\n",
    "# Create the model\n",
    "model = ModifiedMLP(input_size, hidden_sizes, output_size, activation_function)\n",
    "# Print the layers in the layers ModuleList object\n",
    "print(model.layers)\n",
    "# Train the model\n",
    "train_model(model, num_epochs=10)\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can try changing the activation function in the hidden layer to the ReLU activation using 3 layers of size 64 and 20 epochs.\n",
    "Is the result better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28  # MNIST images are 28x28 pixels\n",
    "output_size = 10  # 10 classes for digits 0 to 9\n",
    "activation_function = nn.ReLU\n",
    "hidden_sizes = [64, 64, 64]\n",
    "model = ModifiedMLP(input_size, hidden_sizes, output_size, activation_function)\n",
    "train_model(model, num_epochs=20)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with more configuration and hyperparameter values (learning rate, number of layers, layer sizes, activation functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
