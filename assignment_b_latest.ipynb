{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AccentSpectrogramDataset(Dataset):\n",
    "    def __init__(self, folder_path,\n",
    "                 target_sr: int = 16000,\n",
    "                 use_mel: bool = False,\n",
    "                 n_fft: int = 400,\n",
    "                 hop_length: int = None,\n",
    "                 n_mels: int = 64,\n",
    "                 log_scale: bool = True):\n",
    "        # store file paths only; transform per item\n",
    "        self.file_paths = [\n",
    "            os.path.join(folder_path, f)\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.endswith('.wav')\n",
    "        ]\n",
    "        self.target_sr = target_sr\n",
    "        self.use_mel = use_mel\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length or n_fft // 2\n",
    "        self.n_mels = n_mels\n",
    "        self.log_scale = log_scale\n",
    "\n",
    "        # pre-configure transform funct\n",
    "        if self.use_mel:\n",
    "            self._transform = lambda w: T.MelSpectrogram(\n",
    "                sample_rate=self.target_sr,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length,\n",
    "                n_mels=self.n_mels\n",
    "            )(w)\n",
    "        else:\n",
    "            self._transform = lambda w: T.Spectrogram(\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length\n",
    "            )(w)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        if sr != self.target_sr:\n",
    "            waveform = T.Resample(sr, self.target_sr)(waveform)\n",
    "\n",
    "        spec = self._transform(waveform)\n",
    "        if self.log_scale:\n",
    "            spec = torch.log(spec + 1e-6)\n",
    "\n",
    "        fname = os.path.basename(path)\n",
    "        accent = int(fname[0]) - 1          # classes 0â€“4\n",
    "        gender = fname[1]  # 'm' or 'f' \n",
    "        return spec, accent, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# def pad_collate(batch):\n",
    "#     specs, accents = zip(*batch)\n",
    "\n",
    "#     max_len = max([s.shape[-1] for s in specs])\n",
    "#     padded_specs = []\n",
    "\n",
    "#     for s in specs:\n",
    "#         pad_amount = max_len - s.shape[-1]\n",
    "#         padded = F.pad(s, (0, pad_amount))\n",
    "#         padded_specs.append(padded)\n",
    "\n",
    "    # return (\n",
    "#         torch.stack(padded_specs),             # [B, 1, Freq, Time]\n",
    "#         torch.tensor(accents),                # [B]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #baseline\n",
    "import torch.nn.functional as F\n",
    "def pad_collate(batch, target_width=208):\n",
    "    specs, accents, genders = zip(*batch)\n",
    "    padded_specs = []\n",
    "    for s in specs:\n",
    "        pad_amount = target_width - s.shape[-1]\n",
    "        if pad_amount > 0:\n",
    "            padded = torch.nn.functional.pad(s, (0, pad_amount))\n",
    "        else:\n",
    "            padded = s[..., :target_width]\n",
    "        padded_specs.append(padded)\n",
    "    return (\n",
    "        torch.stack(padded_specs),\n",
    "        torch.tensor(accents),\n",
    "        list(genders)   # <--- returns a list of 'm'/'f'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3166\n",
      "Spectrogram shape: torch.Size([1, 201, 526])\n",
      "Label: 1\n",
      "Gender: m\n"
     ]
    }
   ],
   "source": [
    "#dataset = AccentSpectrogramDataset(\"/Users/larsheijnen/DL/Train\")\n",
    "dataset = AccentSpectrogramDataset(\"/Users/larsheijnen/DL/Train\")\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "\n",
    "# Look at shape of first spectrogram\n",
    "x, y, z= dataset[6]\n",
    "print(f\"Spectrogram shape: {x.shape}\")\n",
    "print(f\"Label: {y}\")\n",
    "print(f\"Gender: {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrograms: torch.Size([4, 1, 201, 208])\n",
      "Accents: tensor([1, 3, 3, 2])\n",
      "Gender: ['m', 'm', 'f', 'm']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Use batch_size=4 for low RAM, pin_memory is False for macOS/MPS\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=pad_collate, pin_memory=False)\n",
    "\n",
    "# Try again\n",
    "for batch in dataloader:\n",
    "    spectrograms, accents, gender = batch\n",
    "    print(f\"Spectrograms: {spectrograms.shape}\")  # (B, 1, F, T)\n",
    "    print(f\"Accents: {accents}\")                  # (B,)\n",
    "    print(f\"Gender: {gender}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Model 1 (baseline)\n",
    "class CNNBaseline(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "    \n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "#Model 2 (baseline + batch normalization)\n",
    "class CNNBaseline_BatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "#Model 3 (baseline + dropout 0.3)\n",
    "class CNNBaseline_Dropout3(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "#Model 4 (baseline + dropout 0.5)\n",
    "class CNNBaseline_Dropout5(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.dropout = nn.Dropout(dropout_p) \n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "#Model 5 (baseline + bacth normalization + dropout 0.3)\n",
    "class CNNBaseline_Dropout3_BatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.dropout = nn.Dropout(dropout_p) \n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "#Model 6 (baseline + bacth normalization + dropout 0.5)\n",
    "class CNNBaseline_Dropout5_BatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))  \n",
    "        self.dropout = nn.Dropout(dropout_p) \n",
    "        self.fc = nn.Linear(32 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AccentCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5, dropout_p: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(8)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(16)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(32)\n",
    "        self.pool3 = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.fc = nn.Linear(32, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)               # â†’ (B, 32)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"Model1\": CNNBaseline,\n",
    "    \"Model2\": CNNBaseline_BatchNorm, \n",
    "    \"Model3\": CNNBaseline_Dropout3,\n",
    "    \"Model4\": CNNBaseline_Dropout5,\n",
    "    \"Model5\": CNNBaseline_Dropout3_BatchNorm,\n",
    "    \"Model6\": CNNBaseline_Dropout5_BatchNorm,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training model: CNNBaseline ===\n",
      "Epoch  1 | Train Loss: 903.057 | Train Acc: 67.81% | Train Prec: 66.51% | Train Recall: 66.25% | Train F1: 66.07% || Test Acc: 61.51% | Test Prec: 61.55% | Test Recall: 61.01% | Test F1: 61.02%\n",
      "Epoch  2 | Train Loss: 408.834 | Train Acc: 84.28% | Train Prec: 85.87% | Train Recall: 83.11% | Train F1: 82.56% || Test Acc: 74.61% | Test Prec: 77.94% | Test Recall: 74.07% | Test F1: 72.61%\n",
      "Epoch  3 | Train Loss: 248.124 | Train Acc: 91.90% | Train Prec: 91.22% | Train Recall: 91.45% | Train F1: 91.22% || Test Acc: 80.91% | Test Prec: 80.87% | Test Recall: 80.32% | Test F1: 80.22%\n",
      "Epoch  4 | Train Loss: 168.604 | Train Acc: 92.77% | Train Prec: 93.04% | Train Recall: 92.28% | Train F1: 92.52% || Test Acc: 79.65% | Test Prec: 81.22% | Test Recall: 78.80% | Test F1: 79.51%\n",
      "Epoch  5 | Train Loss: 106.352 | Train Acc: 95.02% | Train Prec: 94.74% | Train Recall: 94.72% | Train F1: 94.61% || Test Acc: 83.12% | Test Prec: 83.49% | Test Recall: 82.88% | Test F1: 82.80%\n",
      "\n",
      "Classification Report for CNNBaseline:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.917     0.957     0.936       138\n",
      "           1      0.844     0.873     0.858       118\n",
      "           2      0.926     0.725     0.813       120\n",
      "           3      0.826     0.793     0.810       150\n",
      "           4      0.662     0.796     0.723       108\n",
      "\n",
      "    accuracy                          0.831       634\n",
      "   macro avg      0.835     0.829     0.828       634\n",
      "weighted avg      0.840     0.831     0.832       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline:\n",
      "Male: {'accuracy': 0.8280254777070064, 'precision': 0.8361980262024092, 'recall': 0.8347547791861978, 'f1': 0.8327485033444575}\n",
      "Female: {'accuracy': 0.834375, 'precision': 0.8339337635564934, 'recall': 0.8110006026338061, 'f1': 0.8155003420003565}\n",
      "\n",
      "=== Training model: CNNBaseline_BatchNorm ===\n",
      "Epoch  1 | Train Loss: 950.975 | Train Acc: 53.36% | Train Prec: 60.18% | Train Recall: 49.15% | Train F1: 47.49% || Test Acc: 46.06% | Test Prec: 53.26% | Test Recall: 43.20% | Test F1: 41.24%\n",
      "Epoch  2 | Train Loss: 531.038 | Train Acc: 80.41% | Train Prec: 80.74% | Train Recall: 78.92% | Train F1: 79.06% || Test Acc: 70.66% | Test Prec: 70.53% | Test Recall: 69.89% | Test F1: 69.06%\n",
      "Epoch  3 | Train Loss: 271.075 | Train Acc: 91.79% | Train Prec: 91.33% | Train Recall: 91.17% | Train F1: 91.14% || Test Acc: 82.18% | Test Prec: 82.73% | Test Recall: 81.89% | Test F1: 81.93%\n",
      "Epoch  4 | Train Loss: 204.758 | Train Acc: 92.93% | Train Prec: 92.96% | Train Recall: 92.38% | Train F1: 92.42% || Test Acc: 82.33% | Test Prec: 83.61% | Test Recall: 81.33% | Test F1: 81.66%\n",
      "Epoch  5 | Train Loss: 134.573 | Train Acc: 94.83% | Train Prec: 95.20% | Train Recall: 94.05% | Train F1: 94.46% || Test Acc: 82.18% | Test Prec: 83.02% | Test Recall: 81.15% | Test F1: 81.29%\n",
      "\n",
      "Classification Report for CNNBaseline_BatchNorm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.787     0.993     0.878       138\n",
      "           1      0.925     0.831     0.875       118\n",
      "           2      0.806     0.833     0.820       120\n",
      "           3      0.795     0.827     0.810       150\n",
      "           4      0.838     0.574     0.681       108\n",
      "\n",
      "    accuracy                          0.822       634\n",
      "   macro avg      0.830     0.811     0.813       634\n",
      "weighted avg      0.827     0.822     0.817       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline_BatchNorm:\n",
      "Male: {'accuracy': 0.821656050955414, 'precision': 0.8171028495738575, 'recall': 0.8239652519566644, 'f1': 0.8163145509608528}\n",
      "Female: {'accuracy': 0.821875, 'precision': 0.8483569528526876, 'recall': 0.796195594935481, 'f1': 0.8070362751835181}\n",
      "\n",
      "=== Training model: CNNBaseline_Dropout3 ===\n",
      "Epoch  1 | Train Loss: 825.831 | Train Acc: 77.57% | Train Prec: 77.29% | Train Recall: 75.97% | Train F1: 76.24% || Test Acc: 68.14% | Test Prec: 67.98% | Test Recall: 67.05% | Test F1: 66.82%\n",
      "Epoch  2 | Train Loss: 371.757 | Train Acc: 88.19% | Train Prec: 87.94% | Train Recall: 87.75% | Train F1: 87.48% || Test Acc: 77.60% | Test Prec: 78.14% | Test Recall: 77.35% | Test F1: 76.59%\n",
      "Epoch  3 | Train Loss: 200.575 | Train Acc: 91.71% | Train Prec: 91.62% | Train Recall: 91.34% | Train F1: 91.17% || Test Acc: 79.65% | Test Prec: 80.63% | Test Recall: 78.85% | Test F1: 78.74%\n",
      "Epoch  4 | Train Loss: 122.450 | Train Acc: 94.87% | Train Prec: 95.45% | Train Recall: 93.72% | Train F1: 94.31% || Test Acc: 79.81% | Test Prec: 80.85% | Test Recall: 78.45% | Test F1: 78.66%\n",
      "Epoch  5 | Train Loss: 96.057 | Train Acc: 97.83% | Train Prec: 97.66% | Train Recall: 97.85% | Train F1: 97.71% || Test Acc: 84.07% | Test Prec: 85.72% | Test Recall: 83.68% | Test F1: 84.15%\n",
      "\n",
      "Classification Report for CNNBaseline_Dropout3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.877     0.934       138\n",
      "           1      0.967     0.754     0.848       118\n",
      "           2      0.898     0.883     0.891       120\n",
      "           3      0.749     0.873     0.806       150\n",
      "           4      0.672     0.796     0.729       108\n",
      "\n",
      "    accuracy                          0.841       634\n",
      "   macro avg      0.857     0.837     0.842       634\n",
      "weighted avg      0.859     0.841     0.845       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline_Dropout3:\n",
      "Male: {'accuracy': 0.856687898089172, 'precision': 0.8696262929596262, 'recall': 0.8616029563659129, 'f1': 0.8640456564204804}\n",
      "Female: {'accuracy': 0.825, 'precision': 0.8501242581935553, 'recall': 0.8187399400120408, 'f1': 0.8219641576723571}\n",
      "\n",
      "=== Training model: CNNBaseline_Dropout5 ===\n",
      "Epoch  1 | Train Loss: 964.548 | Train Acc: 42.10% | Train Prec: 45.88% | Train Recall: 38.72% | Train F1: 33.61% || Test Acc: 37.38% | Test Prec: 30.20% | Test Recall: 35.05% | Test F1: 29.28%\n",
      "Epoch  2 | Train Loss: 714.001 | Train Acc: 71.92% | Train Prec: 73.06% | Train Recall: 72.76% | Train F1: 70.77% || Test Acc: 62.46% | Test Prec: 65.22% | Test Recall: 63.39% | Test F1: 61.88%\n",
      "Epoch  3 | Train Loss: 399.739 | Train Acc: 84.83% | Train Prec: 86.53% | Train Recall: 83.90% | Train F1: 83.70% || Test Acc: 73.03% | Test Prec: 76.71% | Test Recall: 72.91% | Test F1: 72.49%\n",
      "Epoch  4 | Train Loss: 253.277 | Train Acc: 89.85% | Train Prec: 90.34% | Train Recall: 88.42% | Train F1: 89.06% || Test Acc: 76.81% | Test Prec: 77.93% | Test Recall: 75.61% | Test F1: 75.91%\n",
      "Epoch  5 | Train Loss: 172.252 | Train Acc: 95.10% | Train Prec: 94.66% | Train Recall: 95.15% | Train F1: 94.80% || Test Acc: 81.55% | Test Prec: 81.26% | Test Recall: 81.57% | Test F1: 81.19%\n",
      "\n",
      "Classification Report for CNNBaseline_Dropout5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.941     0.920     0.930       138\n",
      "           1      0.800     0.881     0.839       118\n",
      "           2      0.748     0.867     0.803       120\n",
      "           3      0.822     0.707     0.760       150\n",
      "           4      0.752     0.704     0.727       108\n",
      "\n",
      "    accuracy                          0.815       634\n",
      "   macro avg      0.813     0.816     0.812       634\n",
      "weighted avg      0.818     0.815     0.814       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline_Dropout5:\n",
      "Male: {'accuracy': 0.802547770700637, 'precision': 0.8065785458130759, 'recall': 0.8144421138290857, 'f1': 0.8052250653125478}\n",
      "Female: {'accuracy': 0.828125, 'precision': 0.8164439324812696, 'recall': 0.8191677546307453, 'f1': 0.8143486550295688}\n",
      "\n",
      "=== Training model: CNNBaseline_Dropout3_BatchNorm ===\n",
      "Epoch  1 | Train Loss: 907.918 | Train Acc: 58.93% | Train Prec: 63.13% | Train Recall: 54.26% | Train F1: 51.66% || Test Acc: 48.90% | Test Prec: 50.23% | Test Recall: 45.92% | Test F1: 41.73%\n",
      "Epoch  2 | Train Loss: 528.492 | Train Acc: 87.56% | Train Prec: 87.24% | Train Recall: 86.56% | Train F1: 86.59% || Test Acc: 74.45% | Test Prec: 75.24% | Test Recall: 73.79% | Test F1: 73.49%\n",
      "Epoch  3 | Train Loss: 273.345 | Train Acc: 93.64% | Train Prec: 93.44% | Train Recall: 93.13% | Train F1: 93.24% || Test Acc: 82.18% | Test Prec: 83.16% | Test Recall: 81.31% | Test F1: 81.77%\n",
      "Epoch  4 | Train Loss: 168.190 | Train Acc: 95.38% | Train Prec: 95.42% | Train Recall: 94.80% | Train F1: 94.89% || Test Acc: 81.39% | Test Prec: 81.73% | Test Recall: 80.70% | Test F1: 80.08%\n",
      "Epoch  5 | Train Loss: 100.088 | Train Acc: 93.80% | Train Prec: 93.27% | Train Recall: 93.72% | Train F1: 93.33% || Test Acc: 79.34% | Test Prec: 79.85% | Test Recall: 79.18% | Test F1: 79.17%\n",
      "\n",
      "Classification Report for CNNBaseline_Dropout3_BatchNorm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.953     0.891     0.921       138\n",
      "           1      0.846     0.746     0.793       118\n",
      "           2      0.782     0.775     0.778       120\n",
      "           3      0.809     0.760     0.784       150\n",
      "           4      0.603     0.787     0.683       108\n",
      "\n",
      "    accuracy                          0.793       634\n",
      "   macro avg      0.799     0.792     0.792       634\n",
      "weighted avg      0.807     0.793     0.797       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline_Dropout3_BatchNorm:\n",
      "Male: {'accuracy': 0.8121019108280255, 'precision': 0.8248465396136366, 'recall': 0.8207000080952813, 'f1': 0.8186249183971684}\n",
      "Female: {'accuracy': 0.775, 'precision': 0.7777644539247421, 'recall': 0.7567440081644458, 'f1': 0.7605492384562152}\n",
      "\n",
      "=== Training model: CNNBaseline_Dropout5_BatchNorm ===\n",
      "Epoch  1 | Train Loss: 890.366 | Train Acc: 66.94% | Train Prec: 68.21% | Train Recall: 64.64% | Train F1: 64.66% || Test Acc: 56.47% | Test Prec: 57.25% | Test Recall: 55.36% | Test F1: 54.06%\n",
      "Epoch  2 | Train Loss: 459.974 | Train Acc: 91.43% | Train Prec: 90.70% | Train Recall: 91.12% | Train F1: 90.87% || Test Acc: 79.02% | Test Prec: 78.67% | Test Recall: 78.69% | Test F1: 78.67%\n",
      "Epoch  3 | Train Loss: 217.788 | Train Acc: 92.54% | Train Prec: 92.03% | Train Recall: 92.60% | Train F1: 92.10% || Test Acc: 77.60% | Test Prec: 78.09% | Test Recall: 77.38% | Test F1: 77.38%\n",
      "Epoch  4 | Train Loss: 118.650 | Train Acc: 95.77% | Train Prec: 95.46% | Train Recall: 95.97% | Train F1: 95.48% || Test Acc: 82.49% | Test Prec: 83.90% | Test Recall: 82.02% | Test F1: 82.07%\n",
      "Epoch  5 | Train Loss: 72.429 | Train Acc: 97.16% | Train Prec: 96.75% | Train Recall: 97.44% | Train F1: 97.01% || Test Acc: 83.60% | Test Prec: 83.73% | Test Recall: 83.79% | Test F1: 83.35%\n",
      "\n",
      "Classification Report for CNNBaseline_Dropout5_BatchNorm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.942     0.949     0.946       138\n",
      "           1      0.882     0.822     0.851       118\n",
      "           2      0.785     0.883     0.831       120\n",
      "           3      0.900     0.720     0.800       150\n",
      "           4      0.677     0.815     0.739       108\n",
      "\n",
      "    accuracy                          0.836       634\n",
      "   macro avg      0.837     0.838     0.834       634\n",
      "weighted avg      0.846     0.836     0.837       634\n",
      "\n",
      "\n",
      "Gender breakdown for CNNBaseline_Dropout5_BatchNorm:\n",
      "Male: {'accuracy': 0.8503184713375797, 'precision': 0.8577233877233879, 'recall': 0.8573763332387203, 'f1': 0.8543598323333729}\n",
      "Female: {'accuracy': 0.821875, 'precision': 0.8150095168818574, 'recall': 0.810898846433758, 'f1': 0.8069663567776775}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Prepare dataset & split\n",
    "dataset = AccentSpectrogramDataset(\n",
    "    '/Users/larsheijnen/DL/Train',\n",
    "    target_sr=16000,\n",
    "    use_mel=True,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    n_mels=64,\n",
    "    log_scale=True)\n",
    "\n",
    "train_len = int(0.8 * len(dataset))\n",
    "test_len  = len(dataset) - train_len\n",
    "train_ds, test_ds = random_split(dataset, [train_len, test_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,  collate_fn=pad_collate)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "device    = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# General (not by gender) evaluation helper\n",
    "def evaluate(loader, model, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for specs, labels, _ in loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            outputs = model(specs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "    acc    = accuracy_score(all_labels, all_preds)\n",
    "    prec   = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1     = f1_score(all_labels, all_preds, average='macro')\n",
    "    return acc, prec, recall, f1\n",
    "\n",
    "# Gender-based evaluation helper\n",
    "def evaluate_by_gender(loader, model, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_genders = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for specs, labels, genders in loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            outputs = model(specs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            all_genders.extend(genders)\n",
    "    results = {}\n",
    "    for gender in ['m', 'f']:\n",
    "        idxs = [i for i, g in enumerate(all_genders) if g == gender]\n",
    "        gender_preds = [all_preds[i] for i in idxs]\n",
    "        gender_labels = [all_labels[i] for i in idxs]\n",
    "        acc = accuracy_score(gender_labels, gender_preds)\n",
    "        prec = precision_score(gender_labels, gender_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(gender_labels, gender_preds, average='macro')\n",
    "        f1 = f1_score(gender_labels, gender_preds, average='macro')\n",
    "        results[gender] = {'accuracy': acc, 'precision': prec, 'recall': recall, 'f1': f1}\n",
    "    return results\n",
    "\n",
    "def classification_report_for_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for specs, labels, _ in loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            outputs = model(specs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "    print(classification_report(all_labels, all_preds, digits=3))\n",
    "\n",
    "for model_name, model_class in models_dict.items():\n",
    "    model = model_class().to(device)\n",
    "    print(f\"\\n=== Training model: {type(model).__name__} ===\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for specs, labels, genders in train_loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(specs), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Compute and print general metrics for this epoch (not by gender)\n",
    "        train_acc, train_prec, train_recall, train_f1 = evaluate(train_loader, model, device)\n",
    "        test_acc, test_prec, test_recall, test_f1 = evaluate(test_loader, model, device)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:2d} | \"\n",
    "            f\"Train Loss: {running_loss:.3f} | \"\n",
    "            f\"Train Acc: {train_acc*100:5.2f}% | \"\n",
    "            f\"Train Prec: {train_prec*100:5.2f}% | \"\n",
    "            f\"Train Recall: {train_recall*100:5.2f}% | \"\n",
    "            f\"Train F1: {train_f1*100:5.2f}% || \"\n",
    "            f\"Test Acc: {test_acc*100:5.2f}% | \"\n",
    "            f\"Test Prec: {test_prec*100:5.2f}% | \"\n",
    "            f\"Test Recall: {test_recall*100:5.2f}% | \"\n",
    "            f\"Test F1: {test_f1*100:5.2f}%\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nClassification Report for {type(model).__name__}:\")\n",
    "    classification_report_for_model(model, test_loader, device)\n",
    "\n",
    "    print(f\"\\nGender breakdown for {type(model).__name__}:\")\n",
    "    gender_results = evaluate_by_gender(test_loader, model, device)\n",
    "    for gender in gender_results:\n",
    "        label = \"Male\" if gender == \"m\" else \"Female\"\n",
    "        print(f\"{label}: {gender_results[gender]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier de code om het beste model toe te passen op de \"echt\" test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
