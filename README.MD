# Accent Classification with Deep Learning

This repository contains two main Jupyter notebooks for accent classification using deep learning on raw audio and spectrogram representations:

- **assignment_a.ipynb**: Approach A â€” Raw waveform classification (1D CNNs)  
- **assignment_b_latest-2.ipynb**: Approach B â€” Spectrogram-based classification (2D CNNs)

Both approaches use PyTorch and torchaudio, and are designed for multi-class accent classification from short audio clips.

---

## ğŸ“ Table of Contents

- [Project Structure](#project-structure)  
- [Dataset Description](#dataset-description)  
- [Approach A: Raw Waveform Classification](#approach-a-raw-waveform-classification)  
- [Approach B: Spectrogram Classification](#approach-b-spectrogram-classification)  
- [Model Training & Evaluation](#model-training--evaluation)  
- [Results & Findings](#results--findings)  
- [Setup & Installation](#setup--installation)  
- [How to Run](#how-to-run)  
- [Saved Models](#saved-models)  
- [References](#references)  

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ assignment_a.ipynb
â”œâ”€â”€ assignment_b_latest-2.ipynb
â”œâ”€â”€ Train/                # Training audio files (WAV), excluded from Github
â”œâ”€â”€ Test set/             # Test audio files (WAV), excluded from Github
â”œâ”€â”€ saved_models/
â”‚   â”œâ”€â”€ A/                # Models from Approach A (1D CNNs)
â”‚   â””â”€â”€ B/                # Models from Approach B (2D CNNs)
â”œâ”€â”€ EDA.ipynb             # Exploratory Data Analysis
â””â”€â”€ ...
```

---

## ğŸ§ Dataset Description

- **ğŸ“ Location**: All training data is in `Train/`. Test files are in `Test set/`.
- **ğŸ”¤ Naming Convention**: Files are named `[accent][gender]_[id].wav` (e.g., `1m_1234.wav`)
  - `accent`: 1â€“5 (class label)  
  - `gender`: `m` (male) or `f` (female)
- **ğŸ¼ Audio Properties**:
  - Total clips: 3,166  
  - Sampling rate: 16,000 Hz  
  - Duration: ~1.7s to ~13s (avg. ~5.3s)  
  - Speakers: 10 (5 male, 5 female)
- **ğŸ§ª Train/Validation Split**:
  - 80% training, 20% validation  
- **ğŸš« Note**: Files in `Test set/` are not used during training; their naming may not match expectations.

---

## ğŸ§ª Approach A: Raw Waveform Classification

- **ğŸ““ Notebook**: `assignment_a.ipynb`  
- **ğŸ§ Input**: Raw waveforms  
- **âš™ï¸ Preprocessing**:
  - Resample to 16kHz  
  - Convert to mono  
  - Standardize (zero mean, unit variance)  
  - Pad/crop to fixed length  
- **ğŸ§  Model Variants**:
  - Baseline  
  - + BatchNorm  
  - + Dropout (0.3, 0.5)  
  - + BatchNorm & Dropout  
- **Training**: 10 epochs (report: up to 150), Adam optimizer, cross-entropy loss  
- **Evaluation**: Accuracy, precision, recall, F1-score, gender-specific metrics

---

## ğŸ§ª Approach B: Spectrogram Classification

- **ğŸ““ Notebook**: `assignment_b_latest-2.ipynb`  
- **ğŸ–¼ï¸ Input**: Mel-spectrograms  
- **âš™ï¸ Preprocessing**:
  - Resample to 16kHz  
  - Compute Mel-spectrogram  
  - Apply log-scaling  
  - Pad/crop to fixed width  
- **ğŸ§  Model Variants**:
  - Baseline  
  - + BatchNorm  
  - + Dropout (0.3, 0.5)  
  - + BatchNorm & Dropout  
- **Training**: 10â€“150 epochs, Adam optimizer, cross-entropy loss  
- **Evaluation**: Full classification reports by accent and gender

---

## ğŸ“ˆ Model Training & Evaluation

- **ğŸ“Š Splits**: 80% train, 20% validation  
- **âš™ï¸ Batch Size**: 4  
- **ğŸ–¥ï¸ Device**: Uses MPS (Apple Silicon) if available, otherwise CPU  
- **ğŸ“Š Metrics**:
  - Accuracy, precision, recall, F1-score  
  - Gender-specific performance  
  - Error analysis by class

---

## ğŸ Results & Findings

### Approach A (1D CNN on Raw Waveforms)
- **Best Validation Accuracy**: ~21%  
- **Observation**: Struggled to learn directly from waveforms  
- **Gender Breakdown**: Male ~15%, Female ~29%  

### Approach B (2D CNN on Mel-Spectrograms)
- **Best Validation Accuracy**: ~87%  
- **Observation**: Spectrogram-based models performed significantly better  
- **Gender Breakdown**: Male ~85%, Female ~84%

*See the notebooks for detailed classification reports.*

---

## âš™ï¸ Setup & Installation

Main dependencies:
- Python 3.11+  
- torch, torchaudio  
- numpy, pandas, matplotlib  
- scikit-learn, librosa, mutagen  

**Install with pip**:

```bash
# (Recommended) Create virtual environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install torch torchaudio numpy pandas matplotlib scikit-learn librosa mutagen
```

---

## â–¶ï¸ How to Run

1. **Place Data**: Ensure all `.wav` files are in `Train/`  
2. **Open Notebook**: Use JupyterLab or VSCode to open either notebook  
3. **Check Paths**: Confirm data paths match (default is relative paths)  
4. **Run Cells**: Top to bottom â€” load data, define models, train, evaluate  
5. **Output**: Trained models saved in `saved_models/`  

---

## ğŸ’¾ Saved Models

- `saved_models/A/`: 1D CNNs (raw waveform, Approach A)  
- `saved_models/B/`: 2D CNNs (spectrograms, Approach B)  
- Includes variants with/without BatchNorm and Dropout

---

## ğŸ”— References

- [PyTorch](https://pytorch.org/)  
- [torchaudio](https://pytorch.org/audio/stable/index.html)  
- [Librosa](https://librosa.org/doc/latest/index.html)

---

## ğŸ“ Notes

- For best performance: use Approach B with BatchNorm + Dropout  
- Data augmentation improves results (see report)  
- Reproducibility: fixed random seeds used for data splits  