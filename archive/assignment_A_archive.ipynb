{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 file_path  \\\n",
      "0  /Users/larsheijnen/DL/Train/2m_9039.wav   \n",
      "1  /Users/larsheijnen/DL/Train/4f_1887.wav   \n",
      "2  /Users/larsheijnen/DL/Train/4f_9571.wav   \n",
      "3  /Users/larsheijnen/DL/Train/1m_3736.wav   \n",
      "4  /Users/larsheijnen/DL/Train/1m_3078.wav   \n",
      "\n",
      "                                            waveform  accent gender  \n",
      "0  [[tensor(-0.0001), tensor(-0.0001), tensor(-5....       2      m  \n",
      "1  [[tensor(0.), tensor(4.4749e-05), tensor(0.), ...       4      f  \n",
      "2  [[tensor(-0.0001), tensor(-0.0002), tensor(-0....       4      f  \n",
      "3  [[tensor(-0.0003), tensor(-0.0003), tensor(-0....       1      m  \n",
      "4  [[tensor(-0.0008), tensor(-0.0009), tensor(-0....       1      m  \n",
      "torch.Size([1, 41400])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_preprocess_audios_from_folder(folder_path, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Load and normalize all audio files in a folder using torchaudio, extracting accent and gender from filename.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to folder containing .wav files\n",
    "        target_sr (int): Sampling rate\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns ['file_path', 'waveform', 'accent', 'gender']\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith('.wav'):\n",
    "            file_path = os.path.join(folder_path, fname)\n",
    "            # Load audio\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "            # Normalize amplitude\n",
    "            waveform = waveform / waveform.abs().max()\n",
    "            # Extract accent and gender\n",
    "            accent = int(fname[0])  # 1-5\n",
    "            gender = fname[1]       # 'm' or 'f'\n",
    "            data.append({\n",
    "                'file_path': file_path,\n",
    "                'waveform': waveform,\n",
    "                'accent': accent,\n",
    "                'gender': gender\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = load_and_preprocess_audios_from_folder(\"/Users/larsheijnen/DL/Train\")\n",
    "print(df.head())\n",
    "\n",
    "#Size first waveform\n",
    "print(df['waveform'].iloc[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Convert accent from 1-5 to 0-4\n",
    "if 'accent' in df.columns:\n",
    "    df['accent_label'] = df['accent'].apply(lambda x: int(x) - 1)\n",
    "else:\n",
    "    print(\"Warning: 'accent' column not found in DataFrame. Accent prediction will not work.\")\n",
    "    # Add a dummy label if you want to proceed with model structure testing\n",
    "    df['accent_label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Device Configuration ---\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Voor macos met M-chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. PyTorch Dataset ---\n",
    "class AccentDatasetRNN(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Waveform is already a tensor [1, length] from your preprocessing\n",
    "        waveform = self.df.iloc[idx]['waveform'].squeeze(0) # RNN expects [seq_len, features] or [batch, seq_len, features]\n",
    "                                                           # Here, seq_len is audio length, features is 1 (mono)\n",
    "        accent_label = self.df.iloc[idx]['accent']\n",
    "        # Convert accent 1-5 to 0-4 for CrossEntropyLoss\n",
    "        label = torch.tensor(accent_label - 1, dtype=torch.long)\n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Custom Collate Function ---\n",
    "def collate_fn_rnn(batch):\n",
    "    # batch is a list of tuples (waveform_tensor, label_tensor)\n",
    "    waveforms = [item[0] for item in batch]\n",
    "    labels = torch.stack([item[1] for item in batch])\n",
    "    \n",
    "    # Get lengths of each sequence\n",
    "    lengths = torch.tensor([len(w) for w in waveforms])\n",
    "\n",
    "    # Pad sequences in this batch (batch_first=False for pack_padded_sequence easier handling with RNN)\n",
    "    # pad_sequence expects a list of tensors, each tensor is a sequence\n",
    "    padded_waveforms = pad_sequence(waveforms, batch_first=False, padding_value=0.0)\n",
    "    # padded_waveforms will be of shape (max_seq_len_in_batch, batch_size)\n",
    "    # We need it as (max_seq_len_in_batch, batch_size, num_features=1) for RNN\n",
    "    padded_waveforms = padded_waveforms.unsqueeze(-1) # Add feature dimension\n",
    "\n",
    "    return padded_waveforms, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. RNN Model Definition ---\n",
    "class AudioRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, num_classes=5, rnn_type='LSTM', dropout=0.3):\n",
    "        super(AudioRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type.upper()\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                               batch_first=False, dropout=dropout if num_layers > 1 else 0)\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                              batch_first=False, dropout=dropout if num_layers > 1 else 0)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x shape: (max_seq_len, batch_size, input_size)\n",
    "        # lengths shape: (batch_size)\n",
    "\n",
    "        # Pack sequence\n",
    "        # Enforce_sorted=False because DataLoader with shuffle=True might not guarantee order\n",
    "        # Sorting is done within collate_fn if needed, but pack_padded_sequence can handle unsorted if lengths are provided\n",
    "        packed_input = pack_padded_sequence(x, lengths.cpu(), batch_first=False, enforce_sorted=False)\n",
    "        \n",
    "        # RNN forward pass\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            packed_output, (hn, cn) = self.rnn(packed_input)\n",
    "        else: # GRU\n",
    "            packed_output, hn = self.rnn(packed_input)\n",
    "        \n",
    "        # We can use the hidden state of the last layer (hn)\n",
    "        # hn is (num_layers, batch_size, hidden_size)\n",
    "        # We want the output of the last RNN layer for each sequence\n",
    "        # Take the hidden state of the last layer:\n",
    "        last_hidden = hn[-1] # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        out = self.dropout(last_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on mps with batch size 16...\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Training Setup ---\n",
    "# Hyperparameters\n",
    "input_size = 1  # Mono audio\n",
    "hidden_size = 128 # Can be tuned\n",
    "num_rnn_layers = 2 # Can be tuned\n",
    "num_classes = 5 # 5 accent classes\n",
    "learning_rate = 0.001\n",
    "batch_size = 16 # Adjust based on your 8GB RAM. Start small.\n",
    "num_epochs = 10 # Start with a few epochs\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "# Ensure 'df' is your DataFrame with 'waveform' and 'accent' columns\n",
    "if 'df' not in locals():\n",
    "    print(\"DataFrame 'df' not found. Please load your data.\")\n",
    "    # df = load_and_preprocess_audios_from_folder(\"/Users/larsheijnen/DL/Train\") # Placeholder\n",
    "else:\n",
    "    train_dataset = AccentDatasetRNN(df)\n",
    "    train_loader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, \n",
    "                              collate_fn=collate_fn_rnn,\n",
    "                              pin_memory=False, # pin_memory is more relevant for CUDA\n",
    "                              num_workers=0 # <--- CHANGE THIS\n",
    "                             )\n",
    "\n",
    "    # Instantiate model, loss, and optimizer\n",
    "    model = AudioRNN(input_size=input_size, \n",
    "                     hidden_size=hidden_size, \n",
    "                     num_layers=num_rnn_layers, \n",
    "                     num_classes=num_classes,\n",
    "                     rnn_type='GRU' # GRU is often a bit faster and lighter than LSTM\n",
    "                    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"Starting training on {device} with batch size {batch_size}...\")\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, (waveforms, lengths, labels) in enumerate(train_loader):\n",
    "            waveforms = waveforms.to(device) \n",
    "            # lengths are already tensors, should be on CPU for pack_padded_sequence\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(waveforms, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 10 == 0: # Print every 10 batches\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] completed. Average Training Loss: {avg_loss:.4f}')\n",
    "\n",
    "    print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
